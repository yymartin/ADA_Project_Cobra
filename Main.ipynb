{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A time Story of offshore Societies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the aim of this project in the README file.\n",
    "\n",
    "Our work is based on 4 databases, namely the *bahamas leaks*, the *panama papers*, the *offshore leaks* and the *paradise papers*. They all contain csv files with all the data of a graph : nodes (one file for each type of node), and edges. We merged all these files in this notebook.\n",
    "\n",
    "You will see all the steps of our data exploration. In the end our objective is to arrive to two Dataframe, one containing all the nodes and the relevant information, and one containing the edges with relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx #For graphs\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import *\n",
    "import dateutil.parser\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import folium\n",
    "import json\n",
    "from folium import IFrame\n",
    "import base64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://www.occrp.org/en/panamapapers/database\n",
    "# TRUMP OFFSHORE INC. is good example to see all entities interacting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames / paths\n",
    "\n",
    "The data is separated for every leak source. For each leak source there is a folder containing the nodes of the graph, that can be of different types : <i>intermediary, officer, entity, address</i> (and <i>other</i> for paradise papers only). The folder also contains the edges of this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahamas_folder = \"bahamas/\"\n",
    "panama_folder = \"panama/\"\n",
    "paradise_folder = \"paradise/\"\n",
    "offshore_folder = \"offshore/\"\n",
    "\n",
    "sources_names = ['bahamas', 'panama', 'paradise', 'offshore']\n",
    "\n",
    "panama_name = panama_folder + \"panama_papers\"\n",
    "paradise_name = paradise_folder + \"paradise_papers\"\n",
    "offshore_name = offshore_folder + \"offshore_leaks\"\n",
    "bahamas_name = bahamas_folder + \"bahamas_leaks\"\n",
    "\n",
    "edges_name = \".edges\"\n",
    "nodes_name = \".nodes.\"\n",
    "\n",
    "address_name = \"address\"\n",
    "intermediary_name = \"intermediary\"\n",
    "officer_name = \"officer\"\n",
    "entity_name = \"entity\"\n",
    "others_name = \"other\" # Only for paradise paper there is this extra entity\n",
    "\n",
    "usual_entity_names = [address_name, intermediary_name, officer_name, entity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build local storage\n",
    "\n",
    "We store data in dictionnaries that map each leak source to its content, which is a dictionnary that maps each type of entity to the Dataframe containing its values. For example <b>d_sources[\"bahamas\"][\"officer\"]</b> is the Dataframe of officers coming from the bahamas leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(filename) :\n",
    "    \"\"\" To have same rules when reading data from csv \"\"\"\n",
    "    return pd.read_csv(filename, dtype = str)\n",
    "\n",
    "def build_dict(source_name):\n",
    "    \"\"\"\n",
    "    Create a dictionnary for a certain source_name (among : Panama papers, Paradise papers...)\n",
    "    that maps to each entity name (among : Officer, Intermediary, Address...)\n",
    "    the content of the csv from source_name for this entity\n",
    "    \"\"\"\n",
    "    d = {en : my_read_csv(source_name + nodes_name + en + \".csv\") for en in usual_entity_names}\n",
    "    \n",
    "    if source_name == paradise_name: # Extra \"other\" entity in paradise papers\n",
    "        d[others_name] = my_read_csv(source_name + nodes_name + others_name + \".csv\")\n",
    "    \n",
    "    #Add edges\n",
    "    d[\"edges\"] = my_read_csv(source_name + edges_name + \".csv\")\n",
    "              \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionnary, that maps each source to its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources = dict()\n",
    "d_sources[\"bahamas\"] = build_dict(bahamas_name)\n",
    "d_sources[\"panama\"] = build_dict(panama_name)\n",
    "d_sources[\"paradise\"] = build_dict(paradise_name)\n",
    "d_sources[\"offshore\"] = build_dict(offshore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources['panama']['entity'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some coloring for printing\n",
    "\n",
    "Keep the same coloring during the project, it makes data very easily readable once you get familiar with the coloring !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = '\\033[1m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RED = '\\033[91m'\n",
    "END = '\\033[0m'\n",
    "\n",
    "color_dict = dict()\n",
    "color_dict[\"bahamas\"] = YELLOW\n",
    "color_dict[\"paradise\"] = GREEN\n",
    "color_dict[\"panama\"] = RED\n",
    "color_dict[\"offshore\"] = BLUE\n",
    "\n",
    "def color(str):\n",
    "    \"\"\"\n",
    "    Returns the str given in the color of the source it is from \n",
    "    (the str must contain source name)\n",
    "    \"\"\"\n",
    "    for source in color_dict.keys():\n",
    "        if source in str:\n",
    "            return color_dict[source] + str + END \n",
    "        \n",
    "    return BOLD + str + END #Default color is BOLD\n",
    "\n",
    "for name, _ in color_dict.items():\n",
    "    print(color(name))\n",
    "print(color(\"Unknown source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what data source misses which column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    for source_compare, dict_data_compare in d_sources.items():\n",
    "        print(\"\\n\", color(source_compare), \"missing columns from source :\", color(source))\n",
    "        for entity in usual_entity_names:\n",
    "            missing_columns = []\n",
    "            for col in dict_data[entity].columns:\n",
    "                if not col in dict_data_compare[entity].columns:\n",
    "                    missing_columns.append(col)\n",
    "            if(len(missing_columns) > 0):\n",
    "                print(\"Node type\", entity, \"misses\", len(missing_columns), \"columns, namely : \", missing_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that <span style=\"color:orange\">bahamas</span> is the most \"complete\" source, in the sense it is the one that has the biggest number of columns missing in the others. We will therefore use it to explore the content of columns. *'inactivation_date'* and  *'struck_off_date'* columns from entity will then be explored in <span style=\"color:red\">panama</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special case : Paradise paper, <i>other</i> node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources[\"paradise\"][\"other\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SourceID in different sources\n",
    "\n",
    "We see paradise papers is the only source that has different sourceID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    print(\"\\nSource :\", color(source))\n",
    "    for entity in usual_entity_names:\n",
    "        value_count =  dict_data[entity][\"sourceID\"].value_counts()\n",
    "        print(\"Node :\", entity, len(value_count), \"different sourceID :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if node_id is a good index for Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_node_id = pd.Series()\n",
    "\n",
    "for source, dict_data in d_sources.items():\n",
    "    merged_node_id_source = pd.Series()\n",
    "    for entity in usual_entity_names:\n",
    "        \n",
    "        merged_node_id_source = merged_node_id_source.append(dict_data[entity][\"node_id\"], ignore_index = True)\n",
    "        \n",
    "        if not dict_data[entity][\"node_id\"].is_unique:\n",
    "            print(\"node_id isn't unique for source\", color(source, \"node\", entity))\n",
    "                  \n",
    "    if not merged_node_id_source.is_unique:\n",
    "        print(\"node_id isn't unique between nodes from source\", color(source))\n",
    "    \n",
    "    merged_node_id = merged_node_id.append(merged_node_id_source.drop_duplicates())\n",
    "\n",
    "if merged_node_id.is_unique:\n",
    "    print(\"node_id is unique between unique nodes from all sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each node type indepently node_id is a good index. Therefore (node_id, node_type) could be a good index (node_type being amond officer, intermediary...)\n",
    "\n",
    "Now explore nodes with same node_id in offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(usual_entity_names)):\n",
    "    for j in range(i+1, len(usual_entity_names)):\n",
    "\n",
    "        left_node = usual_entity_names[i]\n",
    "        node = usual_entity_names[j]\n",
    "        print(color(left_node), color(node))\n",
    "        \n",
    "        if left_node != node:\n",
    "\n",
    "            left = d_sources[\"offshore\"][left_node].set_index(\"node_id\")\n",
    "            right = d_sources[\"offshore\"][node].set_index(\"node_id\")\n",
    "\n",
    "            intersection = left.join(right, on = \"node_id\", how = 'inner', \\\n",
    "                                     lsuffix = \"_\" + left_node,rsuffix = \"_\" + node)\n",
    "\n",
    "            if not intersection.empty:\n",
    "                print(\"Intersection of\", color(left_node), \"and\", color(node), \"count is :\")\n",
    "                print(intersection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the intersection on offshore is between officer and intermediary nodes. Let's see if they are the same values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = d_sources[\"offshore\"][\"officer\"].set_index(\"node_id\")\n",
    "right = d_sources[\"offshore\"][\"intermediary\"].set_index(\"node_id\")\n",
    "\n",
    "intersection = left.join(right, on = \"node_id\", how = 'inner', lsuffix = \"_officer\",rsuffix = \"_interm\")\n",
    "\n",
    "intersection.loc[intersection[\"name_officer\"] != intersection[\"name_interm\"]].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we understand that if someone appears in two different node types, it means it is the same person, but has two roles. This is why in further analysis we will store the pair (node_id, role) as index, because it is unique. We have to add a column to nodes, containing the node type, let's call it label. We saw in the column exploration that bahamas has an equivalent column *labels(n)*, that the other's don't, we'll rename it to *label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep necessary columns, Shape data to our need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_clean = dict()\n",
    "\n",
    "#maps every node type to the columns to keep\n",
    "d_columns = dict()\n",
    "d_columns['address'] = ['country_codes', 'node_id']\n",
    "d_columns['entity'] = ['node_id','name','jurisdiction','incorporation_date']\n",
    "d_columns['intermediary'] = ['node_id', 'country_codes','name']\n",
    "d_columns['officer'] = ['node_id', 'country_codes','name']\n",
    "d_columns['other'] = ['node_id', 'country_codes','name']\n",
    "\n",
    "\n",
    "for source, d in d_sources.items():\n",
    "    \n",
    "    d_clean[source] = dict()\n",
    "    \n",
    "    for node_type in usual_entity_names:\n",
    "        d_clean[source][node_type] = d[node_type][d_columns[node_type]]\n",
    "        d_clean[source][node_type]['source'] = source\n",
    "        d_clean[source][node_type]['type'] = node_type\n",
    "        d_clean[source][node_type]['node_id'] = d_clean[source][node_type]['node_id'].astype(np.int32)\n",
    "    \n",
    "    columns_edges = ['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date']        \n",
    "    columns_edges_bahamas = ['node_1', 'node_2', 'rel_type', 'start_date', 'end_date']    \n",
    "    \n",
    "    if source == \"bahamas\": # adapt different column names\n",
    "        d_clean[source]['edges'] = d_sources[source]['edges'][columns_edges_bahamas]\n",
    "        d_clean[source]['edges'].columns = columns_edges\n",
    "        \n",
    "    else :\n",
    "        d_clean[source]['edges'] = d_sources[source]['edges'][columns_edges]\n",
    "    \n",
    "    d_clean[source]['edges']['source'] = source\n",
    "    d_clean[source]['edges'][\"START_ID\"] = d_clean[source]['edges'][\"START_ID\"].astype(np.int32)\n",
    "    d_clean[source]['edges'][\"END_ID\"] = d_clean[source]['edges'][\"END_ID\"].astype(np.int32) \n",
    "\n",
    "d_clean['paradise']['other'] = d_sources['paradise']['other'][d_columns['other']]\n",
    "d_clean[\"paradise\"]['other']['source'] = 'paradise'\n",
    "d_clean[\"paradise\"]['other']['type'] = 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for countries and jurisdictions\n",
    "\n",
    "These dictionaries map the abrevation of countries to their full name, this way we can drop the longer column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = dict()\n",
    "jurisdictions = dict()\n",
    "\n",
    "for s in sources_names:\n",
    "    for t in usual_entity_names:\n",
    "        countries.update(dict(zip(d_sources[s][t]['country_codes'], d_sources[s][t]['countries'])))\n",
    "        if t  == 'entity':\n",
    "            jurisdictions.update(dict(zip(d_sources[s][t]['jurisdiction'], d_sources[s][t]['jurisdiction_description'])))\n",
    "            \n",
    "countries.update(dict(zip(d_sources['paradise']['other']['country_codes'],\\\n",
    "                          d_sources['paradise']['other']['countries'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and study *node* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pourcentage function to print pourcentages in a nice way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pourcentage(n, precision = 2):\n",
    "    \"\"\" To print a pourcentage in a nice way and with a given precision\"\"\"\n",
    "    return color((\"%.\" + str(precision) + \"f\") % (100.0*n) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to convert string of date to datetime format. There are a LOT of different string formats and we cover most of them. Dates with ambiguity such as 01/03/2001 are treated arbitrarily. (i.e. is this date 1st of March or 3rd of January ?) Indeed the year is generally what matters the most for us.\n",
    "\n",
    "Years are valid until 2015 at most, and starting in the 1960s according to wikipedia (https://en.wikipedia.org/wiki/Panama_Papers)\n",
    "\n",
    "When date is clearly an outlier (18/19/2006), it is set to NaN, and printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date):\n",
    "    \"\"\" Parsing of the date, read above for more details\"\"\"\n",
    "    if (date==date):\n",
    "        try:\n",
    "            formatted = dateutil.parser.parse(date)\n",
    "            if (formatted.year > 2015 or formatted.year < 1960):\n",
    "                formatted = 'NaN'\n",
    "            return formatted \n",
    "        except:\n",
    "            print(date)\n",
    "            return 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node types that should contain NaN for each column name\n",
    "\n",
    "Nodes can have NaN values because of missing data, <b>or</b> because the data doesn't make sense for this node type. You will here find a list of node types for each column, those are node types that are NaN because of this second reason. For example the jurisdiction for an Officer doesn't really make sense at first sight... We will however in the future try to cumpute all the jurisdictions an officer is related to using the edges (and many more)\n",
    "\n",
    "##### name\n",
    "- Address\n",
    "\n",
    "##### jurisdiction and incorporation_date\n",
    "- Officer\n",
    "- Other\n",
    "- Intermediary\n",
    "- Address\n",
    "\n",
    "##### country_codes\n",
    "- Entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(columns=['node_id','source','type','name','country_codes', 'jurisdiction', 'incorporation_date'])\n",
    "\n",
    "for source,_ in d_sources.items():\n",
    "    for node_type in usual_entity_names:\n",
    "        nodes = nodes.append(d_clean[source][node_type], sort=False)\n",
    "#nodes = nodes.append(d_clean['paradise']['other'], sort=False) # Uncomment to consider other nodes\n",
    "\n",
    "nodes.node_id = pd.to_numeric(nodes.node_id, downcast='integer')\n",
    "\n",
    "nodes['incorporation_date'] = nodes['incorporation_date'].apply(parse_date)\n",
    "nodes['incorporation_date'] = pd.to_datetime(nodes['incorporation_date'])\n",
    "\n",
    "nodes[nodes['country_codes'] == 'XXX']['country_codes'] = None\n",
    "nodes[nodes['jurisdiction'] == 'XXX']['jurisdiction'] = None\n",
    "\n",
    "nodes_ = nodes.copy()\n",
    "nodes = nodes.set_index(['node_id', 'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are a lot of unique country_codes... Indeed we notice some nodes have many country codes separated by a ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = nodes.country_codes.dropna()\n",
    "number_multi_country = country_codes[country_codes.str.contains(\";\")].count()\n",
    "\n",
    "print(pourcentage(number_multi_country/len(country_codes)), \"of nodes with a country_code have a country_code with more than one country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study by node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('entity', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('officer', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('intermediary', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('address', level = 1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and study *edges* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.DataFrame(columns=['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date','source'])\n",
    "for source in sources_names:\n",
    "    edges = edges.append(d_clean[source]['edges'], sort=False)\n",
    "    \n",
    "edges.START_ID = pd.to_numeric(edges.START_ID, downcast='integer')\n",
    "edges.END_ID = pd.to_numeric(edges.END_ID, downcast='integer')\n",
    "\n",
    "edges['start_date'] = pd.to_datetime(edges['start_date'].apply(parse_date))\n",
    "edges['end_date'] = pd.to_datetime(edges['end_date'].apply(parse_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed strings are dates that were not read correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are <b>13</b> unique kind of edges, they are listed below. In further analysis it will be interesting to study each of them in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_edges = len(edges)\n",
    "number_start_date = edges.start_date.notna().sum()\n",
    "number_end_date = edges.end_date.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pourcentage(number_start_date/number_edges), \"of edges have a start date\")\n",
    "print(\"Among those,\", pourcentage(number_end_date/(number_end_date+number_start_date)), \"have an end date\")\n",
    "print(\"The first added relation was the\", edges.start_date.min())\n",
    "print(\"The last added relation was the\", edges.start_date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study average and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges.START_ID.value_counts().describe(), '\\n')\n",
    "print(edges.END_ID.value_counts().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that on average <b>2.98</b> edges start with the same node_id, and <b>2.57</b> end with the same node_id\n",
    "\n",
    "Max number of connections starting from a given node is <b>36373</b>, and <b>37338</b> ending (another node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study connection between edges and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_max_links_start = edges.START_ID.value_counts().idxmax()\n",
    "max_links_start = edges.START_ID.value_counts().max()\n",
    "node_max_links_start = nodes.xs(id_max_links_start, level=0)\n",
    "\n",
    "id_max_links_end = edges.END_ID.value_counts().idxmax()\n",
    "max_links_end = edges.END_ID.value_counts().max()\n",
    "node_max_links_end = nodes.xs(id_max_links_end, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The Node with the most START edges has\", color(str(max_links_start)), \"links and is :\")\n",
    "print(node_max_links_start[['source', 'name', 'country_codes']])\n",
    "\n",
    "print(\"\\nThe Node with the most END edges has\", color(str(max_links_end)), \"links and is :\")\n",
    "print(node_max_links_end[['source', 'country_codes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful sets for the following study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nodes = set(edges.START_ID)\n",
    "end_nodes = set(edges.END_ID)\n",
    "\n",
    "nodes_address_ids = set(nodes.xs('address', level='type').index.values)\n",
    "nodes_officer_ids = set(nodes.xs('officer', level='type').index.values)\n",
    "nodes_entity_ids = set(nodes.xs('entity', level='type').index.values)\n",
    "nodes_intermediary_ids = set(nodes.xs('intermediary', level='type').index.values)\n",
    "\n",
    "d_nodes_ids = {\n",
    "    'address': nodes_address_ids,\n",
    "    'officer': nodes_officer_ids,\n",
    "    'entity' : nodes_entity_ids,\n",
    "    'intermediary' : nodes_intermediary_ids\n",
    "}\n",
    "\n",
    "node_ids = frozenset().union(*d_nodes_ids.values())\n",
    "\n",
    "linked_nodes = start_nodes.union(end_nodes)\n",
    "isolated_nodes = node_ids.difference(linked_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study nodes that are isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", color(str(len(isolated_nodes))), \"isolated nodes (0 edge connecting them)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask_isolated = nodes.index.get_level_values(0).isin(isolated_nodes)\n",
    "nodes[mask_isolated].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Milestone 2 end ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update country codes for consistency\n",
    "\n",
    "**For example 'GBR' and 'UK' are the same but have different code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "update_code_dict = dict()\n",
    "\n",
    "wrong_jurisdiction = []\n",
    "\n",
    "for k, v in jurisdictions.items():\n",
    "    if len(str(k)) != 3 and not ';' in str(k):\n",
    "        wrong_jurisdiction.append((k, v))\n",
    "\n",
    "        \n",
    "no_matches_codes = []\n",
    "\n",
    "for k_wrong, v_wrong in wrong_jurisdiction:\n",
    "    \n",
    "    match_found = False\n",
    "    \n",
    "    #Search in jurisdictions another code with matching description\n",
    "    for k, v in jurisdictions.items():\n",
    "        if v == v_wrong and k != k_wrong:\n",
    "            update_code_dict[k_wrong] = k  #Map old key wrong to corresponding one\n",
    "            match_found = True\n",
    "    if not match_found :\n",
    "        #Search in countries another code with matching description\n",
    "        for k, v in countries.items():\n",
    "            if v == v_wrong and k != k_wrong:\n",
    "                update_code_dict[k_wrong] = k  #Map old key wrong to corresponding one\n",
    "                match_found = True\n",
    "    \n",
    "    if not match_found:\n",
    "        no_matches_codes.append((k_wrong, v_wrong))\n",
    "\n",
    "    for k,v in update_code_dict.items():\n",
    "        if len(k) < len(v):\n",
    "            update_code_dict[v] = k\n",
    "            del update_code_dict[k]\n",
    "print(\"No match : \", no_matches_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually convert the ones with no natural match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to find countries or jurisdictions with similar descriptions, custom use\n",
    "\n",
    "descriptions_countries_jurisdictions = list(jurisdictions.values()) + list(countries.values())\n",
    "for desc in descriptions_countries_jurisdictions:\n",
    "    if 'U.' in str(desc) and 'Virgin Islands' in str(desc) :\n",
    "        print(desc)\n",
    "        break\n",
    "jurisdications_inv = {v: k for k, v in jurisdictions.items()}\n",
    "countries_inv = {v: k for k, v in countries.items()}\n",
    "\n",
    "print(jurisdications_inv.get('U.S. Virgin Islands',\"\"))\n",
    "print(countries_inv.get('U.S. Virgin Islands',\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually found equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_code_dict['US'] = 'USA'\n",
    "update_code_dict['CAYMN'] = 'CYM'\n",
    "update_code_dict['STLU'] = 'LCA'\n",
    "update_code_dict['VI'] = 'VIR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the updating map to the nodes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_with_default(x):\n",
    "    return update_code_dict.get(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_['jurisdiction'] = nodes_['jurisdiction'].map(map_with_default, na_action = 'ignore')\n",
    "nodes = nodes_.set_index(['node_id', 'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all content to edges\n",
    "\n",
    "We decided for convenience reasons to add to the edges useful informations contained in the nodes. We therefore build the edges_completed dataframe containing almost all the needed information to answer the questions. To build it we just join it twice, on START_ID then on END_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nodes = nodes.reset_index()[['node_id', 'type', 'country_codes', 'jurisdiction']]\n",
    "merge_nodes.columns = ['id', 'start_type', 'start_country', 'start_jurisdiction']\n",
    "\n",
    "edges_completed = edges.merge(merge_nodes, how = 'left', left_on='START_ID', right_on='id')\n",
    "\n",
    "merge_nodes.columns = ['id', 'end_type', 'end_country', 'end_jurisdiction']\n",
    "\n",
    "edges_completed = edges_completed.merge(merge_nodes, how = 'left', left_on='END_ID', right_on='id')\n",
    "\n",
    "edges_completed = edges_completed.drop(columns = ['id_x', 'id_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful masks\n",
    "\n",
    "Thoses masks are filters to select only edges_completed depending on some explicit criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_code\n",
    "mask_diff_start_end_address = (edges_completed['start_country'] != edges_completed['end_country'])\n",
    "mask_country_notna = (edges_completed['start_country'].notna()) & (edges_completed['end_country'].notna())\n",
    "\n",
    "# jurisdiction\n",
    "mask_start_loc_notna = (edges_completed['start_country'].notna()) |  (edges_completed['start_jurisdiction'].notna())\n",
    "mask_end_loc_notna = (edges_completed['end_country'].notna()) |  (edges_completed['end_jurisdiction'].notna())\n",
    "\n",
    "# intermediary\n",
    "mask_intermediary_start = (edges_completed.start_type == 'intermediary')\n",
    "mask_not_intermediary_start = (edges_completed.start_type != 'intermediary')\n",
    "mask_intermediary_end = (edges_completed.end_type == 'intermediary')\n",
    "mask_intermediary = mask_intermediary_end | mask_intermediary_start\n",
    "\n",
    "# officer\n",
    "mask_officer_start = (edges_completed.start_type == 'officer')\n",
    "\n",
    "# Relationships\n",
    "\n",
    "mask_officer_to_entity = mask_officer_start & (edges_completed['end_type'] == 'entity')\n",
    "\n",
    "# Edges\n",
    "\n",
    "mask_edge_intemerdiary_of = (edges_completed.TYPE == 'intermediary_of')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks for similar nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_same_name_as = (edges_completed.TYPE == 'same_name_as')\n",
    "mask_same_company_as = (edges_completed.TYPE == 'same_company_as')\n",
    "mask_same_id_as = (edges_completed.TYPE == 'same_id_as')\n",
    "mask_same_as = (edges_completed.TYPE == 'same_as')\n",
    "mask_same_address_as = (edges_completed.TYPE == 'same_address_as')\n",
    "mask_same_intermediary_as = (edges_completed.TYPE == 'same_intermediary_as')\n",
    "mask_similar_company_as = (edges_completed.TYPE == 'similar_company_as') # We looked there are 200 and they're the same\n",
    "mask_probably_same_officer_as = (edges_completed.TYPE == 'probably_same_officer_as')\n",
    "\n",
    "mask_same = (mask_same_name_as | mask_same_company_as | mask_same_id_as | mask_same_as | \\\n",
    "            mask_same_address_as | mask_same_intermediary_as | mask_similar_company_as | \\\n",
    "            mask_probably_same_officer_as) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge nodes connected with \"same...\" relationships\n",
    "\n",
    "**Note :** in the end we decided not to apply the changes of this section. We realized nodes with same_name_as, or same_as... were sometimes connected with other relationships such as officer of etc. This means that even if they have the same name or similar informations, they aren't actually the same person, they just use the same 'cover' name. Therefore all the merge actions are commented in the following part, but we left the analysis.\n",
    "\n",
    "Some nodes are connected with edges with a name such as same_name_as, same_company_as...\n",
    "We decided to merge those nodes, by deleting on of them and merging their content together. When conflict on creation date, we always keep the oldest one. We believe those differences in creation date exist because they come from different sources, and the oldest date is the 'first known' date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_completed.TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_same_as = edges_completed[mask_same][['START_ID', 'END_ID']]\n",
    "\n",
    "\n",
    "start_nodes_same = nodes_.loc[nodes_.node_id.isin(nodes_same_as.START_ID)]\n",
    "end_nodes_same = nodes_.loc[nodes_.node_id.isin(nodes_same_as.END_ID)]\n",
    "\n",
    "same_nodes_merge = nodes_same_as.merge(start_nodes_same, how='inner', left_on='START_ID', right_on='node_id')\\\n",
    "                                .merge(end_nodes_same, how='inner', left_on='END_ID', right_on='node_id', suffixes=('_start','_end'))\\\n",
    "                                .drop(columns=['START_ID', 'END_ID'])\n",
    "\n",
    "same_nodes_merge.node_id_start = pd.to_numeric(same_nodes_merge.node_id_start, downcast='integer')\n",
    "same_nodes_merge.node_id_end = pd.to_numeric(same_nodes_merge.node_id_end, downcast='integer')\n",
    "\n",
    "same_nodes_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see they are indeed pretty much the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_nodes_diff_type_mask = same_nodes_merge.type_start != same_nodes_merge.type_end\n",
    "same_nodes_diff_type_count = len(same_nodes_merge[same_nodes_diff_type_mask])\n",
    "\n",
    "same_nodes_diff_country_mask = (same_nodes_merge.country_codes_start.notna())\\\n",
    "                            & (same_nodes_merge.country_codes_end.notna()) \\\n",
    "                            &(same_nodes_merge.country_codes_start != same_nodes_merge.country_codes_end)\n",
    "same_nodes_diff_country_count = len(same_nodes_merge[same_nodes_diff_country_mask])\n",
    "\n",
    "same_nodes_diff_jurisdiction_mask = (same_nodes_merge.jurisdiction_end.notna())\\\n",
    "                            & (same_nodes_merge.jurisdiction_start.notna()) \\\n",
    "                            &(same_nodes_merge.jurisdiction_end != same_nodes_merge.jurisdiction_start)\n",
    "same_nodes_diff_jurisdiction_count = len(same_nodes_merge[same_nodes_diff_country_mask])\n",
    "\n",
    "same_nodes_merge = same_nodes_merge[(~same_nodes_diff_type_mask) & \\\n",
    "                                    (~same_nodes_diff_country_mask) & \\\n",
    "                                    (~same_nodes_diff_jurisdiction_mask) ]\n",
    "\n",
    "print(\"Node not to merge : \")\n",
    "print(\"%d nodes that are similar but have not the same type\" % same_nodes_diff_type_count)\n",
    "print(\"%d nodes that are similar but have not the same country\" % same_nodes_diff_country_count)\n",
    "print(\"%d nodes that are similar but have not the same jurisdiction\" % same_nodes_diff_jurisdiction_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two other columns that can differ are source, name and the node_id:\n",
    "- For source we will arbitrarily take the start one, since we don't really care in source analyis in further work.\n",
    "- For the name we will also take the start one, usually differences are very small (caps, or missing letter...)\n",
    "- For the node_id we will also take the start one, and erase the end one in nodes. We will moreover replace its occurences in edges with the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_nodes_merge['merge_date'] = same_nodes_merge[['incorporation_date_start','incorporation_date_end']].min(axis=1)\n",
    "nodes_replace = same_nodes_merge[['node_id_end', 'node_id_start','type_start', 'source_start', 'name_start',\\\n",
    "                         'country_codes_start', 'jurisdiction_start', 'merge_date']]\n",
    "nodes_replace.columns = ['old_node_id', 'node_id', 'type', 'source', 'name', 'country_codes', 'jurisdiction', 'incorporation_date']\n",
    "\n",
    "old_ids_to_replace = set(nodes_replace.old_node_id.unique())\n",
    "\n",
    "# Remove nodes that are both in old and new\n",
    "nodes_replace = nodes_replace[~nodes_replace['node_id'].isin(old_ids_to_replace)]\n",
    "\n",
    "ids_to_update = set(nodes_replace.node_id.unique())\n",
    "\n",
    "dict_old_new_id = dict()\n",
    "\n",
    "for index, row in nodes_replace.iterrows():\n",
    "    dict_old_new_id[row['old_node_id']] = row['node_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete nodes of old_node_id value, and update the other ones with the computed merge value\n",
    "\n",
    "*Uncomment to merge the similar nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes_ = nodes_[~nodes_.node_id.isin(old_ids_to_replace)]\n",
    "#nodes_.update(nodes_replace)\n",
    "#nodes = nodes_.set_index(['node_id', 'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove same* edges from edges_completed**\n",
    "\n",
    "This actually shouldn't be commented, we want to remove the edges that just say some nodes look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed = edges_completed[~mask_same]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update the edges with the old nodes ids to new ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_old_new_default(x):\n",
    "    return dict_old_new_id.get(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had merged the nodes, this would have changed the ID's of edges in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(edges_completed['START_ID'].unique()), 'START_ID no merge')\n",
    "print(len(edges_completed['START_ID'].map(map_old_new_default).unique()), 'START_ID if merge')\n",
    "print(len(edges_completed['END_ID'].unique()), 'END_ID no merge')\n",
    "print(len(edges_completed['END_ID'].map(map_old_new_default).unique()), 'END_ID if merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment to merge the similar nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges_completed['START_ID'] = edges_completed['START_ID'].map(map_old_new_default)\n",
    "#edges_completed['END_ID'] = edges_completed['END_ID'].map(map_old_new_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update the edges with the merger value for new ids**\n",
    "\n",
    "*Uncomment to merge the similar nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update when END_ID is in nodes to replace\n",
    "nodes_replace.columns = ['old_node_id', 'END_ID', 'end_type', 'source', 'name', 'end_country', 'end_jurisdiction', 'incorporation_date']\n",
    "#edges_completed.update(nodes_replace)\n",
    "\n",
    "# Update when START_ID is in nodes to replace\n",
    "nodes_replace.columns = ['old_node_id', 'START_ID', 'start_type', 'source', 'name', 'start_country', 'start_jurisdiction', 'incorporation_date']\n",
    "#edges_completed.update(nodes_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some edges are linking nodes to themselves, we notice most of those edges are intermediary_of type, meaning some nodes were the same people, but they were thaught to be intermediary of one another !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) How does tax evasion evolve over time? Does the number of offshore societies increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are created each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dates = nodes[nodes['incorporation_date'].notna()]\n",
    "entity_dates['incorporation_date'] = entity_dates['incorporation_date'].apply(lambda x: (x.month,x.year))\n",
    "creation_per_month = entity_dates.groupby('incorporation_date').size()\n",
    "total_per_month = creation_per_month.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_per_month.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are there in total each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_month.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are created each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dates['incorporation_date'] = entity_dates['incorporation_date'].apply(lambda x: x[1])\n",
    "creation_per_year = entity_dates.groupby('incorporation_date').size()\n",
    "total_per_year = creation_per_year.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_per_year.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities in total are there each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_per_year.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are created each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_month_year(df):\n",
    "    temp_edges = df.to_dict()\n",
    "    for i in range(1960,2016):\n",
    "        for j in range(1,13):\n",
    "            if (j,i) not in temp_edges:\n",
    "                temp_edges[(j,i)] = 0\n",
    "    return pd.Series(temp_edges).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['start_date'].notna()]\n",
    "edges_date['start_date'] = edges_date['start_date'].apply(lambda x: (x.month,x.year))\n",
    "start_date  = edges_date.groupby('start_date').size()\n",
    "start_date = fill_month_year(start_date)\n",
    "start_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are deleted each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['end_date'].notna()]\n",
    "edges_date['end_date'] = edges_date['end_date'].apply(lambda x: (x.month,x.year))\n",
    "end_date = edges_date.groupby('end_date').size()\n",
    "end_date = fill_month_year(end_date)\n",
    "end_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges in total are there each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_deletion = start_date.subtract(end_date)\n",
    "edges_month_sum = creation_deletion.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "edges_month_sum.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are created each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_year(df):\n",
    "    temp_edges = df.to_dict()\n",
    "    for i in range(1960,2016):\n",
    "        if i not in temp_edges:\n",
    "            temp_edges[i] = 0\n",
    "    return pd.Series(temp_edges).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['start_date'].notna()]\n",
    "edges_date['start_date'] = edges_date['start_date'].apply(lambda x: x.year)\n",
    "start_date  = edges_date.groupby('start_date').size()\n",
    "start_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are deleted each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['end_date'].notna()]\n",
    "edges_date['end_date'] = edges_date['end_date'].apply(lambda x: x.year)\n",
    "end_date = edges_date.groupby('end_date').size()\n",
    "end_date = fill_year(end_date)\n",
    "end_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges in total are there each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_deletion = start_date.subtract(end_date)\n",
    "edges_year_sum = creation_deletion.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_year_sum.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "creation_per_month.plot(ax=ax1, color='lightcoral')\n",
    "ax1.set_title('Number of entities created each month')\n",
    "ax1.set_xlabel('Date (month)')\n",
    "\n",
    "creation_per_year.plot(ax=ax2, color='gold')\n",
    "ax2.set_title('Number of entities created each year')\n",
    "ax2.set_xlabel('Date (year)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "start_date.plot(ax=ax1, color='lightskyblue')\n",
    "ax1.set_title('Number of edges created each year')\n",
    "ax1.set_xlabel('Date (year)')\n",
    "end_date.plot(ax=ax2, color='yellowgreen')\n",
    "ax2.set_title('Number of edges deleted each year')\n",
    "ax2.set_xlabel('Date (year)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = edges_year_sum.plot(figsize=(10,5), color='lightcoral')\n",
    "fig.set_title('Total number of edges with respect to the year')\n",
    "fig.set_xlabel('Date (year)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Do people that are related to one offshore account tend to be related to many more? In other words, is it more likely to have another account once you already have one, than it is to have at least one account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some custom made functions to make nice plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pourcentage = 0.05\n",
    "\n",
    "def my_autopct(pct):\n",
    "    \"\"\"\n",
    "    Only print pct if it is more than threshold\n",
    "    \"\"\"\n",
    "    return ('%1.1f%%' % pct) if pct > 100*threshold_pourcentage else ''\n",
    "\n",
    "def my_labels(serie):\n",
    "    \"\"\"\n",
    "    Label is the jurisdiction name except if it's lower than threshold then it's ''\n",
    "    \"\"\"\n",
    "    total = serie.sum().values[0]\n",
    "    return [jurisdictions[row[0]] if row[1].values/total > 0.05 else '' for row in serie.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histedges_equalN(x, nbin):\n",
    "    \"\"\"\n",
    "        To build bins of equal height (the length will vary)\n",
    "    \"\"\"\n",
    "    npt = len(x)\n",
    "    return np.interp(np.linspace(0, npt, nbin + 1),\n",
    "                     np.arange(npt),\n",
    "                     np.sort(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see officer are related to entites are always the START_ID of the edge relationships. This is why to study them we filter on edge going from *officer* to *entity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_officer_to_entity = edges_completed[mask_officer_to_entity]\n",
    "entities_per_officer = edges_officer_to_entity['START_ID'].value_counts()\n",
    "\n",
    "plt.xlim(1, 15)\n",
    "plt.ylim(0.7, 1)\n",
    "#plt.xscale('log')\n",
    "plt.hist(entities_per_officer, bins=[1, 2, 10, 100, 1000, 10000, 100000], cumulative = True, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.hist(entities_per_officer.values, bins = 150, log=True, color='lightcoral')\n",
    "plt.title('# entities per officer')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_officers = len(entities_per_officer)\n",
    "\n",
    "pourcentage_one_account = len(entities_per_officer[entities_per_officer == 1]) / total_officers * 100\n",
    "pourcentage_two_account = len(entities_per_officer[entities_per_officer <= 2]) / total_officers * 100\n",
    "pourcentage_ten_account = len(entities_per_officer[entities_per_officer <= 10]) / total_officers * 100\n",
    "\n",
    "\n",
    "print(entities_per_officer.mean())\n",
    "print('%.2f%% of officer are related to only one account' % pourcentage_one_account)\n",
    "print('%.2f%% of officer are related to two or less account' % pourcentage_two_account)\n",
    "print('%.2f%% of officer are related to ten or less account' % pourcentage_ten_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = entities_per_officer.values\n",
    "cat1 = entities_per_officer[entities_per_officer == 1].values\n",
    "cat2 = entities_per_officer[(entities_per_officer > 1) & (entities_per_officer < 5)].values\n",
    "cat3 = entities_per_officer[(entities_per_officer >= 5) & (entities_per_officer < 10)].values\n",
    "cat4 = entities_per_officer[entities_per_officer >= 10].values\n",
    "\n",
    "categories = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "#fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#-----\n",
    "index = range(len(categories))\n",
    "bar_width = 0.35\n",
    "rects = ax1.bar(index, categories, color=['lightcoral','gold', 'lightskyblue', 'yellowgreen'])\n",
    "\n",
    "labels = categories\n",
    "\n",
    "low = min(categories)\n",
    "high = max(categories)\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    ax1.text(x = index[i]-0.1, y = categories[i]+0.025*(high-low), s = labels[i], size = 9)\n",
    "\n",
    "ax1.set_title('# entities per officer')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_xticklabels(('[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['))\n",
    "ax1.set_ylim([0, high+0.1*(high-low)])\n",
    "\n",
    "#-------\n",
    "\n",
    "sizes = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "labels = '[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['\n",
    "colors = ['lightcoral','gold', 'lightskyblue', 'yellowgreen']\n",
    "explode = (0.1, 0, 0, 0)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=my_autopct, shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "ax2.set_title('prop. # entities per officer', pad=20)\n",
    "\n",
    "#-------\n",
    "fig.set_size_inches(18, 5)\n",
    "#ax.set_xticklabels(('A', 'B', 'C', 'D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = entities_per_officer.values\n",
    "cat1 = entities_per_officer[(entities_per_officer > 1) & (entities_per_officer < 5)].values\n",
    "cat2 = entities_per_officer[(entities_per_officer >= 5) & (entities_per_officer < 10)].values\n",
    "cat3 = entities_per_officer[entities_per_officer >= 10].values\n",
    "\n",
    "categories = [len(cat1), len(cat2), len(cat3)]\n",
    "#fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#-----\n",
    "index = range(len(categories))\n",
    "bar_width = 0.35\n",
    "rects = ax1.bar(index, categories, color=['lightcoral','gold', 'lightskyblue'])\n",
    "\n",
    "labels = categories\n",
    "\n",
    "low = min(categories)\n",
    "high = max(categories)\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    ax1.text(x = index[i]-0.1, y = categories[i]+0.025*(high-low), s = labels[i], size = 9)\n",
    "\n",
    "ax1.set_title('# entities per officer')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_xticklabels((']1, 5]', ']5, 10]', '[10, inf['))\n",
    "ax1.set_ylim([0, high+0.1*(high-low)])\n",
    "\n",
    "#-------\n",
    "\n",
    "sizes = [len(cat1), len(cat2), len(cat3)]\n",
    "labels = ']1, 5]', ']5, 10]', '[10, inf['\n",
    "colors = ['lightcoral','gold', 'lightskyblue']\n",
    "#explode = (0.1, 0, 0, 0)\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct=my_autopct, shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "ax2.set_title('prop. # entities per officer', pad=20)\n",
    "\n",
    "#-------\n",
    "fig.set_size_inches(18, 5)\n",
    "#ax.set_xticklabels(('A', 'B', 'C', 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Do many people share one offshore society or do they tend to have their own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question we build a dataframe of connected components to entity. For each entity we gather the IDs of the edges connected to it. Entity is always on the END part of the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_entities = nodes_[nodes_.type == 'entity'].node_id.to_frame()\n",
    "\n",
    "entities_connections = nodes_entities.merge(edges_completed, how = 'left', left_on='node_id', right_on='END_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *entities_connections* : contains all the edges that have an entity as an end_type\n",
    "- *connections_per_entity* : contains the number of connected nodes for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_per_entity = entities_connections.node_id.value_counts()\n",
    "\n",
    "type_connected_entity = entities_connections.start_type.value_counts()\n",
    "plt.pie(type_connected_entity, labels=type_connected_entity.index, autopct='%2.f%%')\n",
    "plt.title('Distribution of node types connected to entites')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few entities connected to an entity. They are noted as registered address, and must be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.hist(connections_per_entity.values, bins = 150, log=True, color='lightcoral')\n",
    "plt.title('# connections per entity')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = connections_per_entity\n",
    "cat1 = connections_per_entity[connections_per_entity == 1]\n",
    "cat2 = connections_per_entity[(connections_per_entity > 1) & (connections_per_entity < 5)]\n",
    "cat3 = connections_per_entity[(connections_per_entity >= 5) & (connections_per_entity < 10)]\n",
    "cat4 = connections_per_entity[connections_per_entity >= 10]\n",
    "\n",
    "categories = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "#fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#-----\n",
    "index = range(len(categories))\n",
    "bar_width = 0.35\n",
    "rects = ax1.bar(index, categories, color=['lightcoral','gold', 'lightskyblue', 'yellowgreen'])\n",
    "\n",
    "labels = categories\n",
    "\n",
    "low = min(categories)\n",
    "high = max(categories)\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    ax1.text(x = index[i]-0.1, y = categories[i]+0.025*(high-low), s = labels[i], size = 9)\n",
    "\n",
    "ax1.set_title('# connections per entity')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_xticklabels(('[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['))\n",
    "ax1.set_ylim([0, high+0.1*(high-low)])\n",
    "\n",
    "#-------\n",
    "\n",
    "sizes = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "labels = '[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['\n",
    "colors = ['lightcoral','gold', 'lightskyblue', 'yellowgreen']\n",
    "explode = (0.1, 0, 0, 0)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=my_autopct, shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "ax2.set_title('prop. # connections per entity', pad=20)\n",
    "\n",
    "#-------\n",
    "fig.set_size_inches(18, 5)\n",
    "#ax.set_xticklabels(('A', 'B', 'C', 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) How does the proportion of intermediaries depend on the category of company (number of connections) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*count_type_entities* : maps each entity to the count of connections of each type (officer / intermediary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_columns = {'END_ID': 'entity' , 'start_type': 'type_connected' , 'START_ID':'id_connected'}\n",
    "\n",
    "count_type_entities = entities_connections[['END_ID', 'start_type', 'START_ID']]\\\n",
    "                                .rename(columns=map_columns)\\\n",
    "                                .groupby(['entity','type_connected']).count()\n",
    "\n",
    "\"\"\"\n",
    "count_type_entities = count_type_entities.join(count_type_entities.reset_index() \\\n",
    "                                              .drop(columns='type_connected') \\\n",
    "                                              .rename(columns={'count':'total'})\\\n",
    "                                              .groupby('entity').sum(), how='left')\n",
    "\n",
    "count_type_entities['proportion'] = count_type_entities['count']/count_type_entities['total']\n",
    "\n",
    "\"\"\" # Optional \n",
    "\n",
    "count_type_entities = count_type_entities.reset_index()\n",
    "count_type_entities = count_type_entities[count_type_entities['type_connected'] != 'entity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a study on the different categories of companies based on the number of connections : [1], [2-4], [5-9], [10+]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_distrib = count_type_entities.drop(columns='entity').groupby('type_connected').sum()\n",
    "\n",
    "cat1_distrib = count_type_entities[count_type_entities.entity.isin(cat1.index.values)]\\\n",
    ".drop(columns='entity').groupby('type_connected').sum()\n",
    "\n",
    "cat2_distrib = count_type_entities[count_type_entities.entity.isin(cat2.index.values)]\\\n",
    ".drop(columns='entity').groupby('type_connected').sum()\n",
    "\n",
    "cat3_distrib = count_type_entities[count_type_entities.entity.isin(cat3.index.values)]\\\n",
    ".drop(columns='entity').groupby('type_connected').sum()\n",
    "\n",
    "cat4_distrib = count_type_entities[count_type_entities.entity.isin(cat4.index.values)]\\\n",
    ".drop(columns='entity').groupby('type_connected').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('All entities')\n",
    "plt.pie(all_distrib, labels=all_distrib.index.values, autopct=\"%0.2f%%\", shadow=True)\n",
    "plt.savefig(\"Q4_all_distrib\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Entites 1 connection')\n",
    "plt.pie(cat1_distrib, labels=cat1_distrib.index.values, autopct=\"%0.2f%%\", shadow=True)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Entites 2-4 connections')\n",
    "plt.pie(cat2_distrib, labels=cat2_distrib.index.values, autopct=\"%0.2f%%\", shadow=True)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Entites 6-9 connections')\n",
    "plt.pie(cat3_distrib, labels=cat3_distrib.index.values, autopct=\"%0.2f%%\", shadow=True)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Entites 10+ connections')\n",
    "plt.pie(cat4_distrib, labels=cat4_distrib.index.values, autopct=\"%0.2f%%\", shadow=True)\n",
    "plt.savefig(\"Q4_distribs\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediary are essential for a offshore companies, they at least need one to communicate with. But having more people in the company doesn't mean you need more intermediaries to do the work. Intermediaries must therefore have a lot of work when connected to a big company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pourcentage(len(nodes_[nodes_.type == 'intermediary']) / len(nodes_)), \"of nodes are intermediary\")\n",
    "intermediaries_in_edges = edges_completed[(edges_completed.start_type == 'intermediary') | (edges_completed.end_type == 'intermediary')]\n",
    "print(pourcentage(len(intermediaries_in_edges)/len(edges_completed)), 'of edges have an intermediary in them')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now interesting to see if intermediaries connected to big companies are connected to less companies in number. In other words : if you're an intermediary of a big company do you only take care of this one ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = entities_connections[['END_ID', 'start_type', 'START_ID']].rename(columns=map_columns)\n",
    "q4 = q4[q4.type_connected != 'entity']\n",
    "\n",
    "q4 = q4.join(q4.groupby('entity').count().id_connected, on = 'entity', rsuffix= '_count')\n",
    "\n",
    "mask_int_q4 = (q4.type_connected == 'intermediary')\n",
    "\n",
    "cat1_intermediaries = q4[(q4.id_connected_count == 1) & mask_int_q4].id_connected\n",
    "cat2_intermediaries = q4[(q4.id_connected_count >= 2) & (q4.id_connected_count <= 4) & mask_int_q4].id_connected\n",
    "cat3_intermediaries = q4[(q4.id_connected_count >= 5) & (q4.id_connected_count <= 9) & mask_int_q4].id_connected\n",
    "cat4_intermediaries = q4[(q4.id_connected_count >= 10) & mask_int_q4].id_connected\n",
    "\n",
    "entities_per_int = q4[mask_int_q4][['id_connected', 'entity']].groupby('id_connected').count().reset_index()\n",
    "\n",
    "cat1_intermediaries_count = entities_per_int[entities_per_int.id_connected.isin(cat1_intermediaries)]\n",
    "cat2_intermediaries_count = entities_per_int[entities_per_int.id_connected.isin(cat2_intermediaries)]\n",
    "cat3_intermediaries_count = entities_per_int[entities_per_int.id_connected.isin(cat3_intermediaries)]\n",
    "cat4_intermediaries_count = entities_per_int[entities_per_int.id_connected.isin(cat4_intermediaries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On average an intermediary is connected to %.1f entities\" % entities_per_int['entity'].mean())\n",
    "print(\"On average an intermediary that is at least connected to an entity of category :\")\n",
    "print(\"- [1] is connected tois connected to %.1f entities\" % cat1_intermediaries_count['entity'].mean())\n",
    "print(\"- [2-4] is connected to %.1f entities\" % cat2_intermediaries_count['entity'].mean())\n",
    "print(\"- [5-9] is connected to %.1f entities\" % cat3_intermediaries_count['entity'].mean())\n",
    "print(\"- [10+] is connected to %.1f entities\" % cat4_intermediaries_count['entity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median number of connection per intermediary is %.1f\" % entities_per_int['entity'].mean())\n",
    "print(\"The median number of an intermediary at least connected to an entity of category :\")\n",
    "print(\"- [1] is %.1f\" % cat1_intermediaries_count['entity'].median())\n",
    "print(\"- [2-4] is %.1f\" % cat2_intermediaries_count['entity'].median())\n",
    "print(\"- [5-9] is %.1f\" % cat3_intermediaries_count['entity'].median())\n",
    "print(\"- [10+] is %.1f\" % cat4_intermediaries_count['entity'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['All', 'Cat1', 'Cat2', 'Cat3', 'Cat4']\n",
    "data_boxplot = [entities_per_int['entity'], cat1_intermediaries_count['entity'],\\\n",
    "                cat2_intermediaries_count['entity'],\\\n",
    "                cat3_intermediaries_count['entity'], cat4_intermediaries_count['entity']]\n",
    "plt.boxplot(data_boxplot, labels=labels, sym='', showmeans=True, meanline=True)\n",
    "plt.ylim(1, 83)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the mean is being pulled very high because of some extreme values, let's look at them and see statistics without them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "upper_bound = 2000\n",
    "for v in data_boxplot[4]:\n",
    "    if v > upper_bound:\n",
    "        i+=1\n",
    "print(i, \"values are higher than\", upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxplot_cleaned = []\n",
    "for array_values in data_boxplot:\n",
    "    data_boxplot_cleaned.append(list())\n",
    "    for v in array_values:\n",
    "        if v <= upper_bound:\n",
    "            data_boxplot_cleaned[-1].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(data_boxplot_cleaned, labels=labels, sym='', showmeans=True,meanline=True)\n",
    "plt.ylim(1, 50)\n",
    "plt.title('Number of connections per intermediary depending on category')\n",
    "plt.savefig('Q4_boxplot')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** It might sound weird that each one of these averages are all higher individually than the global average.\n",
    "\n",
    "But it actually tells us that intermediaries with many connections have clients (entities) in all of the categories ! So this actually means that the more connections you have as an intermediary, the more diverse they will be.\n",
    "\n",
    "This is the opposite of what we predicted, the bigger entities you intermediate for, the more entities you are connected to. This probably means that some people or groups of people are notorious intermediaries and are therefore working for many entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Is there a correlation between the location of the people and the location of their offshore society?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten country codes with multiple countries separated by a coma\n",
    "\n",
    "Some nodes are related to many countries. This can be the case of big multi-national companies, and their code is written in the form \"AAA;BBB;...;ZZZ\". Here we flatten these values, by create one row for each nationality of each node. This decision is arguable, since it gives the same weight to 10 different nodes as it gives for one node with 10 different nationalities. But since we do this for multi-national companies, we consider they can have many subsidiaries and this is relevant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_diff_start_end_address = edges_completed[mask_country_notna & mask_diff_start_end_address]\\\n",
    ".end_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_q5 = edges_completed[mask_officer_to_entity & mask_start_loc_notna & mask_end_loc_notna][['start_country', 'end_jurisdiction']]\n",
    "edges_q5['sum'] = 1\n",
    "edges_q5.start_country = edges_q5.start_country.str.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_country = []\n",
    "\n",
    "for index, row in edges_q5[(edges_q5.start_country.map(len) > 1)].iterrows():\n",
    "    for country in row['start_country']:\n",
    "        modified_row = row.copy()\n",
    "        modified_row['start_country'] = country + '_GRP'\n",
    "        flattened_country.append(modified_row)\n",
    "\n",
    "edges_q5_flattened = edges_q5[edges_q5['start_country'].map(len) == 1]\n",
    "edges_q5_flattened.start_country = edges_q5_flattened.start_country.apply(lambda arr: arr[0])\n",
    "edges_q5_flattened = edges_q5_flattened.append(flattened_country)\n",
    "      \n",
    "print(len(edges_q5), \"edges when you don't flatten the country codes\")\n",
    "print(len(edges_q5_flattened), \"edges when you flatten the country codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_distrib = edges_q5_flattened.groupby(['start_country', 'end_jurisdiction']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Pie printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pourcentage = 0.05\n",
    "\n",
    "def my_autopct(pct):\n",
    "    \"\"\"\n",
    "    Only print pct if it is more than threshold\n",
    "    \"\"\"\n",
    "    return ('%1.1f%%' % pct) if pct > 100*threshold_pourcentage else ''\n",
    "\n",
    "def my_labels(serie):\n",
    "    \"\"\"\n",
    "    Label is the jurisdiction name except if it's lower than threshold then it's ''\n",
    "    \"\"\"\n",
    "    total = serie.sum().values[0]\n",
    "    return [jurisdictions[row[0]] if row[1].values/total > 0.05 else '' for row in serie.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pie = q5_distrib.xs('USA', level='start_country').sort_values('sum', ascending = False)\n",
    "labels = my_labels(one_pie)\n",
    "\n",
    "plt.pie(one_pie, radius = 2, labels=labels, autopct=my_autopct, startangle=90, shadow = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_distrib_country = q5_distrib.groupby('start_country').sum()\n",
    "q5_distrib_country = q5_distrib.join(q5_distrib_country, how = 'left', on = 'start_country', rsuffix = '_country')\n",
    "q5_distrib_country['pourcentage'] = q5_distrib_country['sum']/q5_distrib_country['sum_country']\n",
    "q5_distrib_country.groupby('start_country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_scatter_pourc = 0.1\n",
    "threshold_scatter_countries = 10000\n",
    "\n",
    "threshold_mask = (q5_distrib_country['pourcentage'] > threshold_scatter_pourc) \\\n",
    "& (q5_distrib_country['sum_country'] > threshold_scatter_countries)\n",
    "\n",
    "q5_thresholded = q5_distrib_country[threshold_mask]\n",
    "\n",
    "countries_x_scatter =  [val[0] for val in q5_thresholded.index.values]\n",
    "jurisdictions_y_scatter =  [jurisdictions[val[1]] for val in q5_thresholded.index.values]\n",
    "scalars_scatter = q5_thresholded['pourcentage'].values * 100\n",
    "\n",
    "plt.scatter(countries_x_scatter, jurisdictions_y_scatter, scalars_scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set thresholds to only print relevant data and not put too much noise in the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_scatter_pourc = 0.05\n",
    "threshold_scatter_countries = 10000\n",
    "\n",
    "threshold_mask = (q5_distrib_country['pourcentage'] > threshold_scatter_pourc) \\\n",
    "& (q5_distrib_country['sum_country'] > threshold_scatter_countries)\n",
    "\n",
    "q5_thresholded = q5_distrib_country[threshold_mask]\n",
    "\n",
    "countries_x_scatter =  [val[0] for val in q5_thresholded.index.values]\n",
    "jurisdictions_y_scatter =  [jurisdictions[val[1]] for val in q5_thresholded.index.values]\n",
    "scalars_scatter = q5_thresholded['pourcentage'].values * 150\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(countries_x_scatter, jurisdictions_y_scatter, scalars_scatter, color=['lightcoral'])\n",
    "plt.title('Correlation between the location of the people and the location of their offshore society')\n",
    "plt.xlabel('Origin of the Officer')\n",
    "plt.ylabel('Location of the Offshore Society')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see Malta and the British Virgin Islands as constant prefered choice whatever the country of origin of the officer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UP cell : now plot distribution for each country and see if there is always a leader, what is the variance, %of total for top1, BUILD MAP Where you can click on a country and see the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed[(edges_completed['start_jurisdiction'] != edges_completed['end_jurisdiction']) \\\n",
    "               & (edges_completed['start_jurisdiction'].notna()) & ((edges_completed['end_jurisdiction'].notna()))]\\\n",
    ".TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot map for 10 biggest countries¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center(code, coord):\n",
    "    if code == 'USA':\n",
    "        return 38.36320700, -101.989278\n",
    "    if code == 'CAN':\n",
    "        return 56.09433049, -101.989278\n",
    "    lat = 0\n",
    "    lon = 0\n",
    "    count = 0\n",
    "    for elem in coord:\n",
    "        lat += elem[0]\n",
    "        lon += elem[1]\n",
    "        count += 1\n",
    "    return [lon/count, lat/count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data = json.load(open('world.json')) \n",
    "code_to_coord = {}\n",
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    coord = world_data['features'][i]['geometry']['coordinates'][0][0]\n",
    "    code_to_coord[code] = compute_center(code, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries = edges_q5_flattened[edges_q5_flattened['start_country'].apply(lambda x: 'GRP' not in x)].groupby('start_country').size().sort_values(ascending = False)\n",
    "top10 = top_countries[0:11]\n",
    "top10 = top10.drop('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(df):\n",
    "    m = df['sum'].sum()\n",
    "    df['sum'] = df['sum'].apply(lambda x: x*100 / m)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylefunction(feature,df):\n",
    "    return {\n",
    "        'fillOpacity': 1 if feature['id'] in df else 0,\n",
    "        'weight': 0,\n",
    "        'fillColor': 'blue'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = folium.Map([2,30], tiles='OpenStreetMap', zoom_start = 2.3)\n",
    "world_data = json.load(open('world.json'))  \n",
    "list_countries = top10.index\n",
    "test = []\n",
    "for code in code_to_coord:\n",
    "    if code in list_countries and code in countries:\n",
    "        coord = code_to_coord[code]\n",
    "        df = percentage(q5_distrib.xs(code, level='start_country'))\n",
    "        arrows = {}\n",
    "        weights = {}\n",
    "        for c in df.index:\n",
    "            if c not in test:\n",
    "                test.append(c)\n",
    "            if c in code_to_coord:\n",
    "                arrows[c] = code_to_coord[c]\n",
    "                weights[c] = min(10,df.xs(c)['sum']/2)\n",
    "                \n",
    "        #df = df.rename(index={'VG': 'BVI', 'CY':'CYP', 'IM':'IOM', 'KY':'CYM', 'MT':'MLT', 'NZ':'NZL',\n",
    "        #                      'PA':'PMA', 'SC':'SEY', 'SG':'SGP', 'US':'USA', 'KNA':'KN', 'COOK':'CK',\n",
    "        #                      'CAYMN':'CYM', 'MARSH':'MH', 'MAURI':'MU', 'DUBAI':'AE', 'BERMU':'BM', 'LIBER':'LR',\n",
    "        #                      'CHINA':'CN', 'NETH':'NL', 'BRB':'BB', 'BS':'BAH', 'NEV':'USA', 'WYO':'USA',\n",
    "        #                     'USDE':'USA'})\n",
    "        ## REPLACE VG WITH BVI, CY WITH CYP, IM WITH IOM, KY WITH CYM, MT WITH MLT, NZ WITH NZL, PA WITH PMA\n",
    "        ## SC WITH SEY, SG WITH SGP, US WITH USA, KNA WITH KN, COOK WITH CK, CAYMN WITH CYM, MARSH WITH MH,\n",
    "        ## MAURI WITH MU, DUBAI WITH AE, BERMU WITH BM, LIBER WITH LR, CHINA WITH CN, NETH WITH NL, BRB WITH BB,\n",
    "        ## BS WITH BAH\n",
    "        \n",
    "        marker = folium.Marker(location=coord, icon = folium.Icon(color='red'), popup=countries[code])\n",
    "        layer = folium.GeoJson(world_data,\n",
    "                   name=countries[code],\n",
    "                   style_function = lambda x : stylefunction(x,df.to_dict()['sum'])\n",
    "                      ).add_child(marker)\n",
    "        \n",
    "        for c in arrows:\n",
    "            if c in countries:\n",
    "                line = folium.PolyLine([coord,arrows[c]],weight=max(0.5,weights[c]))\n",
    "                marker_end = folium.Marker(location=arrows[c], popup=countries[c])\n",
    "                layer.add_child(marker_end)\n",
    "            \n",
    "                layer.add_child(line)\n",
    "        \n",
    "        layer.add_to(world)\n",
    "\n",
    "    \n",
    "\n",
    "folium.LayerControl().add_to(world)\n",
    "\n",
    "world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot map with markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pie_chart(code):\n",
    "    one_pie = q5_distrib.xs(code, level='start_country').sort_values('sum', ascending = False)\n",
    "    labels = my_labels(one_pie)\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    colors = ['lightcoral','gold', 'lightskyblue', 'yellowgreen', 'violet', 'lightgray', 'palegreen', 'pink']\n",
    "    plt.pie(one_pie, labels=labels, colors=colors, autopct=my_autopct, startangle=90, shadow = True)\n",
    "    plt.title(countries[code], fontsize=12, fontweight=\"bold\", pad=5)\n",
    "    png = 'pie_{}.png'.format(code)\n",
    "    plt.savefig('figures/'+png, dpi=75)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = top_countries[0:101]\n",
    "top100 = top100.drop('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data = json.load(open('world.json'))\n",
    "\n",
    "world = folium.Map([2,30], tiles='OpenStreetMap', zoom_start = 2.3)\n",
    "\n",
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    coord = world_data['features'][i]['geometry']['coordinates'][0][0]\n",
    "    if code in top100:\n",
    "        compute_pie_chart(code)\n",
    "        \n",
    "        png = 'figures/pie_{}.png'.format(code)\n",
    "        encoded = base64.b64encode(open(png, 'rb').read()).decode()\n",
    "    \n",
    "        html = '<img src=\"data:image/png;base64,{}\">'.format\n",
    "        iframe = IFrame(html(encoded), width=400, height=200)\n",
    "        popup = folium.Popup(iframe, max_width=2650)\n",
    "    \n",
    "        folium.Marker(location=compute_center(code,coord), popup=popup).add_to(world)\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_name = {}\n",
    "for c in countries:\n",
    "    name = countries[c]\n",
    "    code = c\n",
    "    countries_name[name] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    name = world_data['features'][i]['properties']['name']\n",
    "    if name in countries_name:\n",
    "        if code != countries_name[name]:\n",
    "            world_data['features'][i]['id'] = countries_name[name]\n",
    "    if name not in countries_name:\n",
    "        print(name)\n",
    "\n",
    "with open(\"world.json\", \"w\") as jsonFile:\n",
    "    json.dump(world_data, jsonFile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
