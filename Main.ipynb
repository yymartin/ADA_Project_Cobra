{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A time Story of offshore Societies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the aim of this project in the README file.\n",
    "\n",
    "Our work is based on 4 databases, namely the *bahamas leaks*, the *panama papers*, the *offshore leaks* and the *paradise papers*. They all contain csv files with all the data of a graph : nodes (one file for each type of node), and edges. We merged all these files in this notebook.\n",
    "\n",
    "You will see all the steps of our data exploration. In the end our objective is to arrive to two Dataframe, one containing all the nodes and the relevant information, and one containing the edges with relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "import networkx as nx #For graphs\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import *\n",
    "import dateutil.parser\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import folium\n",
    "import json\n",
    "from folium import IFrame\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://www.occrp.org/en/panamapapers/database\n",
    "# TRUMP OFFSHORE INC. is good example to see all entities interacting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames / paths\n",
    "\n",
    "The data is separated for every leak source. For each leak source there is a folder containing the nodes of the graph, that can be of different types : <i>intermediary, officer, entity, address</i> (and <i>other</i> for paradise papers only). The folder also contains the edges of this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahamas_folder = \"bahamas/\"\n",
    "panama_folder = \"panama/\"\n",
    "paradise_folder = \"paradise/\"\n",
    "offshore_folder = \"offshore/\"\n",
    "\n",
    "sources_names = ['bahamas', 'panama', 'paradise', 'offshore']\n",
    "\n",
    "panama_name = panama_folder + \"panama_papers\"\n",
    "paradise_name = paradise_folder + \"paradise_papers\"\n",
    "offshore_name = offshore_folder + \"offshore_leaks\"\n",
    "bahamas_name = bahamas_folder + \"bahamas_leaks\"\n",
    "\n",
    "edges_name = \".edges\"\n",
    "nodes_name = \".nodes.\"\n",
    "\n",
    "address_name = \"address\"\n",
    "intermediary_name = \"intermediary\"\n",
    "officer_name = \"officer\"\n",
    "entity_name = \"entity\"\n",
    "others_name = \"other\" # Only for paradise paper there is this extra entity\n",
    "\n",
    "usual_entity_names = [address_name, intermediary_name, officer_name, entity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build local storage\n",
    "\n",
    "We store data in dictionnaries that map each leak source to its content, which is a dictionnary that maps each type of entity to the Dataframe containing its values. For example <b>d_sources[\"bahamas\"][\"officer\"]</b> is the Dataframe of officers coming from the bahamas leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(filename) :\n",
    "    \"\"\" To have same rules when reading data from csv \"\"\"\n",
    "    return pd.read_csv(filename, dtype = str)\n",
    "\n",
    "def build_dict(source_name):\n",
    "    \"\"\"\n",
    "    Create a dictionnary for a certain source_name (among : Panama papers, Paradise papers...)\n",
    "    that maps to each entity name (among : Officer, Intermediary, Address...)\n",
    "    the content of the csv from source_name for this entity\n",
    "    \"\"\"\n",
    "    d = {en : my_read_csv(source_name + nodes_name + en + \".csv\") for en in usual_entity_names}\n",
    "    \n",
    "    if source_name == paradise_name: # Extra \"other\" entity in paradise papers\n",
    "        d[others_name] = my_read_csv(source_name + nodes_name + others_name + \".csv\")\n",
    "    \n",
    "    #Add edges\n",
    "    d[\"edges\"] = my_read_csv(source_name + edges_name + \".csv\")\n",
    "              \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionnary, that maps each source to its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources = dict()\n",
    "d_sources[\"bahamas\"] = build_dict(bahamas_name)\n",
    "d_sources[\"panama\"] = build_dict(panama_name)\n",
    "d_sources[\"paradise\"] = build_dict(paradise_name)\n",
    "d_sources[\"offshore\"] = build_dict(offshore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources['panama']['entity'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some coloring for printing\n",
    "\n",
    "Keep the same coloring during the project, it makes data very easily readable once you get familiar with the coloring !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = '\\033[1m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RED = '\\033[91m'\n",
    "END = '\\033[0m'\n",
    "\n",
    "color_dict = dict()\n",
    "color_dict[\"bahamas\"] = YELLOW\n",
    "color_dict[\"paradise\"] = GREEN\n",
    "color_dict[\"panama\"] = RED\n",
    "color_dict[\"offshore\"] = BLUE\n",
    "\n",
    "def color(str):\n",
    "    \"\"\"\n",
    "    Returns the str given in the color of the source it is from \n",
    "    (the str must contain source name)\n",
    "    \"\"\"\n",
    "    for source in color_dict.keys():\n",
    "        if source in str:\n",
    "            return color_dict[source] + str + END \n",
    "        \n",
    "    return BOLD + str + END #Default color is BOLD\n",
    "\n",
    "for name, _ in color_dict.items():\n",
    "    print(color(name))\n",
    "print(color(\"Unknown source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what data source misses which column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    for source_compare, dict_data_compare in d_sources.items():\n",
    "        print(\"\\n\", color(source_compare), \"missing columns from source :\", color(source))\n",
    "        for entity in usual_entity_names:\n",
    "            missing_columns = []\n",
    "            for col in dict_data[entity].columns:\n",
    "                if not col in dict_data_compare[entity].columns:\n",
    "                    missing_columns.append(col)\n",
    "            if(len(missing_columns) > 0):\n",
    "                print(\"Node type\", entity, \"misses\", len(missing_columns), \"columns, namely : \", missing_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that <span style=\"color:orange\">bahamas</span> is the most \"complete\" source, in the sense it is the one that has the biggest number of columns missing in the others. We will therefore use it to explore the content of columns. *'inactivation_date'* and  *'struck_off_date'* columns from entity will then be explored in <span style=\"color:red\">panama</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special case : Paradise paper, <i>other</i> node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources[\"paradise\"][\"other\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SourceID in different sources\n",
    "\n",
    "We see paradise papers is the only source that has different sourceID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    print(\"\\nSource :\", color(source))\n",
    "    for entity in usual_entity_names:\n",
    "        value_count =  dict_data[entity][\"sourceID\"].value_counts()\n",
    "        print(\"Node :\", entity, len(value_count), \"different sourceID :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if node_id is a good index for Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_node_id = pd.Series()\n",
    "\n",
    "for source, dict_data in d_sources.items():\n",
    "    merged_node_id_source = pd.Series()\n",
    "    for entity in usual_entity_names:\n",
    "        \n",
    "        merged_node_id_source = merged_node_id_source.append(dict_data[entity][\"node_id\"], ignore_index = True)\n",
    "        \n",
    "        if not dict_data[entity][\"node_id\"].is_unique:\n",
    "            print(\"node_id isn't unique for source\", color(source, \"node\", entity))\n",
    "                  \n",
    "    if not merged_node_id_source.is_unique:\n",
    "        print(\"node_id isn't unique between nodes from source\", color(source))\n",
    "    \n",
    "    merged_node_id = merged_node_id.append(merged_node_id_source.drop_duplicates())\n",
    "\n",
    "if merged_node_id.is_unique:\n",
    "    print(\"node_id is unique between unique nodes from all sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each node type indepently node_id is a good index. Therefore (node_id, node_type) could be a good index (node_type being amond officer, intermediary...)\n",
    "\n",
    "Now explore nodes with same node_id in offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(usual_entity_names)):\n",
    "    for j in range(i+1, len(usual_entity_names)):\n",
    "\n",
    "        left_node = usual_entity_names[i]\n",
    "        node = usual_entity_names[j]\n",
    "        print(color(left_node), color(node))\n",
    "        \n",
    "        if left_node != node:\n",
    "\n",
    "            left = d_sources[\"offshore\"][left_node].set_index(\"node_id\")\n",
    "            right = d_sources[\"offshore\"][node].set_index(\"node_id\")\n",
    "\n",
    "            intersection = left.join(right, on = \"node_id\", how = 'inner', \\\n",
    "                                     lsuffix = \"_\" + left_node,rsuffix = \"_\" + node)\n",
    "\n",
    "            if not intersection.empty:\n",
    "                print(\"Intersection of\", color(left_node), \"and\", color(node), \"count is :\")\n",
    "                print(intersection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the intersection on offshore is between officer and intermediary nodes. Let's see if they are the same values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = d_sources[\"offshore\"][\"officer\"].set_index(\"node_id\")\n",
    "right = d_sources[\"offshore\"][\"intermediary\"].set_index(\"node_id\")\n",
    "\n",
    "intersection = left.join(right, on = \"node_id\", how = 'inner', lsuffix = \"_officer\",rsuffix = \"_interm\")\n",
    "\n",
    "intersection.loc[intersection[\"name_officer\"] != intersection[\"name_interm\"]].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we understand that if someone appears in two different node types, it means it is the same person, but has two roles. This is why in further analysis we will store the pair (node_id, role) as index, because it is unique. We have to add a column to nodes, containing the node type, let's call it label. We saw in the column exploration that bahamas has an equivalent column *labels(n)*, that the other's don't, we'll rename it to *label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep necessary columns, Shape data to our need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_clean = dict()\n",
    "\n",
    "#maps every node type to the columns to keep\n",
    "d_columns = dict()\n",
    "d_columns['address'] = ['country_codes', 'node_id']\n",
    "d_columns['entity'] = ['node_id','name','jurisdiction','incorporation_date']\n",
    "d_columns['intermediary'] = ['node_id', 'country_codes','name']\n",
    "d_columns['officer'] = ['node_id', 'country_codes','name']\n",
    "d_columns['other'] = ['node_id', 'country_codes','name']\n",
    "\n",
    "\n",
    "for source, d in d_sources.items():\n",
    "    \n",
    "    d_clean[source] = dict()\n",
    "    \n",
    "    for node_type in usual_entity_names:\n",
    "        d_clean[source][node_type] = d[node_type][d_columns[node_type]]\n",
    "        d_clean[source][node_type]['source'] = source\n",
    "        d_clean[source][node_type]['type'] = node_type\n",
    "        d_clean[source][node_type]['node_id'] = d_clean[source][node_type]['node_id'].astype(np.int32)\n",
    "    \n",
    "    columns_edges = ['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date']        \n",
    "    columns_edges_bahamas = ['node_1', 'node_2', 'rel_type', 'start_date', 'end_date']    \n",
    "    \n",
    "    if source == \"bahamas\": # adapt different column names\n",
    "        d_clean[source]['edges'] = d_sources[source]['edges'][columns_edges_bahamas]\n",
    "        d_clean[source]['edges'].columns = columns_edges\n",
    "        \n",
    "    else :\n",
    "        d_clean[source]['edges'] = d_sources[source]['edges'][columns_edges]\n",
    "    \n",
    "    d_clean[source]['edges']['source'] = source\n",
    "    d_clean[source]['edges'][\"START_ID\"] = d_clean[source]['edges'][\"START_ID\"].astype(np.int32)\n",
    "    d_clean[source]['edges'][\"END_ID\"] = d_clean[source]['edges'][\"END_ID\"].astype(np.int32) \n",
    "\n",
    "d_clean['paradise']['other'] = d_sources['paradise']['other'][d_columns['other']]\n",
    "d_clean[\"paradise\"]['other']['source'] = 'paradise'\n",
    "d_clean[\"paradise\"]['other']['type'] = 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for countries and jurisdictions\n",
    "\n",
    "These dictionaries map the abrevation of countries to their full name, this way we can drop the longer column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = dict()\n",
    "jurisdictions = dict()\n",
    "\n",
    "for s in sources_names:\n",
    "    for t in usual_entity_names:\n",
    "        countries.update(dict(zip(d_sources[s][t]['country_codes'], d_sources[s][t]['countries'])))\n",
    "        if t  == 'entity':\n",
    "            jurisdictions.update(dict(zip(d_sources[s][t]['jurisdiction'], d_sources[s][t]['jurisdiction_description'])))\n",
    "            \n",
    "countries.update(dict(zip(d_sources['paradise']['other']['country_codes'],\\\n",
    "                          d_sources['paradise']['other']['countries'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and study *node* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pourcentage function to print pourcentages in a nice way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pourcentage(n, precision = 2):\n",
    "    \"\"\" To print a pourcentage in a nice way and with a given precision\"\"\"\n",
    "    return color((\"%.\" + str(precision) + \"f\") % (100.0*n) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to convert string of date to datetime format. There are a LOT of different string formats and we cover most of them. Dates with ambiguity such as 01/03/2001 are treated arbitrarily. (i.e. is this date 1st of March or 3rd of January ?) Indeed the year is generally what matters the most for us.\n",
    "\n",
    "Years are valid until 2015 at most, and starting in the 1960s according to wikipedia (https://en.wikipedia.org/wiki/Panama_Papers)\n",
    "\n",
    "When date is clearly an outlier (18/19/2006), it is set to NaN, and printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date):\n",
    "    \"\"\" Parsing of the date, read above for more details\"\"\"\n",
    "    if (date==date):\n",
    "        try:\n",
    "            formatted = dateutil.parser.parse(date)\n",
    "            if (formatted.year > 2015 or formatted.year < 1960):\n",
    "                formatted = 'NaN'\n",
    "            return formatted \n",
    "        except:\n",
    "            print(date)\n",
    "            return 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node types that should contain NaN for each column name\n",
    "\n",
    "Nodes can have NaN values because of missing data, <b>or</b> because the data doesn't make sense for this node type. You will here find a list of node types for each column, those are node types that are NaN because of this second reason. For example the jurisdiction for an Officer doesn't really make sense at first sight... We will however in the future try to cumpute all the jurisdictions an officer is related to using the edges (and many more)\n",
    "\n",
    "##### name\n",
    "- Address\n",
    "\n",
    "##### jurisdiction and incorporation_date\n",
    "- Officer\n",
    "- Other\n",
    "- Intermediary\n",
    "- Address\n",
    "\n",
    "##### country_codes\n",
    "- Entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(columns=['node_id','source','type','name','country_codes', 'jurisdiction', 'incorporation_date'])\n",
    "\n",
    "for source,_ in d_sources.items():\n",
    "    for node_type in usual_entity_names:\n",
    "        nodes = nodes.append(d_clean[source][node_type], sort=False)\n",
    "#nodes = nodes.append(d_clean['paradise']['other'], sort=False) # Uncomment to consider other nodes\n",
    "\n",
    "nodes = nodes.astype({'node_id' : int})\n",
    "\n",
    "nodes = nodes.set_index(['node_id', 'type'])\n",
    "nodes['incorporation_date'] = nodes['incorporation_date'].apply(parse_date)\n",
    "nodes['incorporation_date'] = pd.to_datetime(nodes['incorporation_date'])\n",
    "\n",
    "nodes[nodes['country_codes'] == 'XXX']['country_codes'] = None\n",
    "nodes[nodes['jurisdiction'] == 'XXX']['country_codes'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are a lot of unique country_codes... Indeed we notice some nodes have many country codes separated by a ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = nodes.country_codes.dropna()\n",
    "number_multi_country = country_codes[country_codes.str.contains(\";\")].count()\n",
    "\n",
    "print(pourcentage(number_multi_country/len(country_codes)), \"of nodes with a country_code have a country_code with more than one country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study by node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('entity', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('officer', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('intermediary', level = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.xs('address', level = 1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and study *edges* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.DataFrame(columns=['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date','source'])\n",
    "for source in sources_names:\n",
    "    edges = edges.append(d_clean[source]['edges'], sort=False)\n",
    "    \n",
    "edges = edges.astype({'START_ID' : int, 'END_ID' : int})\n",
    "\n",
    "edges['start_date'] = pd.to_datetime(edges['start_date'].apply(parse_date))\n",
    "edges['end_date'] = pd.to_datetime(edges['end_date'].apply(parse_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed strings are dates that were not read correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are <b>13</b> unique kind of edges, they are listed below. In further analysis it will be interesting to study each of them in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_edges = len(edges)\n",
    "number_start_date = edges.start_date.notna().sum()\n",
    "number_end_date = edges.end_date.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pourcentage(number_start_date/number_edges), \"of edges have a start date\")\n",
    "print(\"Among those,\", pourcentage(number_end_date/(number_end_date+number_start_date)), \"have an end date\")\n",
    "print(\"The first added relation was the\", edges.start_date.min())\n",
    "print(\"The last added relation was the\", edges.start_date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study average and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges.START_ID.value_counts().describe(), '\\n')\n",
    "print(edges.END_ID.value_counts().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that on average <b>2.98</b> edges start with the same node_id, and <b>2.57</b> end with the same node_id\n",
    "\n",
    "Max number of connections starting from a given node is <b>36373</b>, and <b>37338</b> ending (another node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study connection between edges and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_max_links_start = edges.START_ID.value_counts().idxmax()\n",
    "max_links_start = edges.START_ID.value_counts().max()\n",
    "node_max_links_start = nodes.xs(id_max_links_start, level=0)\n",
    "\n",
    "id_max_links_end = edges.END_ID.value_counts().idxmax()\n",
    "max_links_end = edges.END_ID.value_counts().max()\n",
    "node_max_links_end = nodes.xs(id_max_links_end, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The Node with the most START edges has\", color(str(max_links_start)), \"links and is :\")\n",
    "print(node_max_links_start[['source', 'name', 'country_codes']])\n",
    "\n",
    "print(\"\\nThe Node with the most END edges has\", color(str(max_links_end)), \"links and is :\")\n",
    "print(node_max_links_end[['source', 'country_codes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful sets for the following study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nodes = set(edges.START_ID)\n",
    "end_nodes = set(edges.END_ID)\n",
    "\n",
    "nodes_address_ids = set(nodes.xs('address', level='type').index.values)\n",
    "nodes_officer_ids = set(nodes.xs('officer', level='type').index.values)\n",
    "nodes_entity_ids = set(nodes.xs('entity', level='type').index.values)\n",
    "nodes_intermediary_ids = set(nodes.xs('intermediary', level='type').index.values)\n",
    "\n",
    "d_nodes_ids = {\n",
    "    'address': nodes_address_ids,\n",
    "    'officer': nodes_officer_ids,\n",
    "    'entity' : nodes_entity_ids,\n",
    "    'intermediary' : nodes_intermediary_ids\n",
    "}\n",
    "\n",
    "node_ids = frozenset().union(*d_nodes_ids.values())\n",
    "\n",
    "linked_nodes = start_nodes.union(end_nodes)\n",
    "isolated_nodes = node_ids.difference(linked_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study nodes that are isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", color(str(len(isolated_nodes))), \"isolated nodes (0 edge connecting them)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask_isolated = nodes.index.get_level_values(0).isin(isolated_nodes)\n",
    "nodes[mask_isolated].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning\n",
    "\n",
    "We are planning to infer using the edges:\n",
    "- all the jurisdictions some nodes such as *officer* have links to, in order to see if see if they are connected to accounts in different jurisdictions.\n",
    "- for *entity* the country_codes, which would be the country of the people that are connected to this jurisdiction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all content to edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[edges['start_date'].notna() & edges['end_date'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nodes = nodes.reset_index()[['node_id', 'type', 'country_codes', 'jurisdiction']]\n",
    "merge_nodes.columns = ['id', 'start_type', 'start_country', 'start_jurisdiction']\n",
    "\n",
    "edges_completed = edges.merge(merge_nodes, how = 'left', left_on='START_ID', right_on='id')\n",
    "\n",
    "merge_nodes.columns = ['id', 'end_type', 'end_country', 'end_jurisdiction']\n",
    "\n",
    "edges_completed = edges_completed.merge(merge_nodes, how = 'left', left_on='END_ID', right_on='id')\n",
    "\n",
    "edges_completed = edges_completed.drop(columns = ['id_x', 'id_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_diff_start_end_address = (edges_completed['start_country'] != edges_completed['end_country'])\n",
    "\n",
    "mask_country_notna = (edges_completed['start_country'].notna()) & (edges_completed['end_country'].notna())\n",
    "\n",
    "mask_officer_to_entity = (edges_completed['start_type'] == 'officer') & (edges_completed['end_type'] == 'entity')\n",
    "\n",
    "mask_start_loc_notna = (edges_completed['start_country'].notna()) |  (edges_completed['start_jurisdiction'].notna())\n",
    "mask_end_loc_notna = (edges_completed['end_country'].notna()) |  (edges_completed['end_jurisdiction'].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) How does tax evasion evolves over time? Does the number of offshore societies increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are created each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dates = nodes[nodes['incorporation_date'].notna()]\n",
    "entity_dates['incorporation_date'] = entity_dates['incorporation_date'].apply(lambda x: (x.month,x.year))\n",
    "creation_per_month = entity_dates.groupby('incorporation_date').size()\n",
    "total_per_month = creation_per_month.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_per_month.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are there in total each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_month.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities are created each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dates['incorporation_date'] = entity_dates['incorporation_date'].apply(lambda x: x[1])\n",
    "creation_per_year = entity_dates.groupby('incorporation_date').size()\n",
    "total_per_year = creation_per_year.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_per_year.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities in total are there each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_per_year.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are created each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_month_year(df):\n",
    "    temp_edges = df.to_dict()\n",
    "    for i in range(1960,2016):\n",
    "        for j in range(1,13):\n",
    "            if (j,i) not in temp_edges:\n",
    "                temp_edges[(j,i)] = 0\n",
    "    return pd.Series(temp_edges).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['start_date'].notna()]\n",
    "edges_date['start_date'] = edges_date['start_date'].apply(lambda x: (x.month,x.year))\n",
    "start_date  = edges_date.groupby('start_date').size()\n",
    "start_date = fill_month_year(start_date)\n",
    "start_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are deleted each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['end_date'].notna()]\n",
    "edges_date['end_date'] = edges_date['end_date'].apply(lambda x: (x.month,x.year))\n",
    "end_date = edges_date.groupby('end_date').size()\n",
    "end_date = fill_month_year(end_date)\n",
    "end_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges in total are there each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_deletion = start_date.subtract(end_date)\n",
    "edges_month_sum = creation_deletion.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "edges_month_sum.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are created each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_year(df):\n",
    "    temp_edges = df.to_dict()\n",
    "    for i in range(1960,2016):\n",
    "        if i not in temp_edges:\n",
    "            temp_edges[i] = 0\n",
    "    return pd.Series(temp_edges).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['start_date'].notna()]\n",
    "edges_date['start_date'] = edges_date['start_date'].apply(lambda x: x.year)\n",
    "start_date  = edges_date.groupby('start_date').size()\n",
    "start_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are deleted each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_date = edges_completed[edges_completed['end_date'].notna()]\n",
    "edges_date['end_date'] = edges_date['end_date'].apply(lambda x: x.year)\n",
    "end_date = edges_date.groupby('end_date').size()\n",
    "end_date = fill_year(end_date)\n",
    "end_date.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges in total are there each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_deletion = start_date.subtract(end_date)\n",
    "edges_year_sum = creation_deletion.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_year_sum.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Do people that are related to one offshore account tend to be related to many more? In other words, is it more likely to have another account once you already have one, than it is to have at least one account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histedges_equalN(x, nbin):\n",
    "    \"\"\"\n",
    "        To build bins of equal height (the length will vary)\n",
    "    \"\"\"\n",
    "    npt = len(x)\n",
    "    return np.interp(np.linspace(0, npt, nbin + 1),\n",
    "                     np.arange(npt),\n",
    "                     np.sort(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_officer_to_entity = edges_completed[mask_officer_to_entity]\n",
    "entities_per_officer = edges_officer_to_entity['START_ID'].value_counts()\n",
    "\n",
    "plt.xlim(1, 15)\n",
    "plt.ylim(0.7, 1)\n",
    "#plt.xscale('log')\n",
    "plt.hist(entities_per_officer, bins=[1, 2, 10, 100, 1000, 10000, 100000], cumulative = True, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_officers = len(entities_per_officer)\n",
    "\n",
    "pourcentage_one_account = len(entities_per_officer[entities_per_officer == 1]) / total_officers * 100\n",
    "pourcentage_two_account = len(entities_per_officer[entities_per_officer <= 2]) / total_officers * 100\n",
    "pourcentage_ten_account = len(entities_per_officer[entities_per_officer <= 10]) / total_officers * 100\n",
    "\n",
    "\n",
    "print(entities_per_officer.mean())\n",
    "print('%.2f%% of officer are related to only one account' % pourcentage_one_account)\n",
    "print('%.2f%% of officer are related to two or less account' % pourcentage_two_account)\n",
    "print('%.2f%% of officer are related to ten or less account' % pourcentage_ten_account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Do many people share one offshore society or do they tend to have their own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_ = nodes.reset_index()\n",
    "\n",
    "nodes_entities = nodes_[nodes_.type == 'entity'].node_id.to_frame()\n",
    "\n",
    "entities_connections = nodes_entities.merge(edges_completed, how = 'left', left_on='node_id', right_on='END_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_per_entity = entities_connections.node_id.value_counts()\n",
    "\n",
    "entities_connections.start_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(connections_per_entity.values, bins = 100)\n",
    "plt.ylim(0, 1100)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = connections_per_entity.values\n",
    "cat1 = connections_per_entity[connections_per_entity == 1].values\n",
    "cat2 = connections_per_entity[(connections_per_entity > 1) & (connections_per_entity < 5)].values\n",
    "cat3 = connections_per_entity[(connections_per_entity >= 5) & (connections_per_entity < 10)].values\n",
    "cat4 = connections_per_entity[connections_per_entity >= 10].values\n",
    "\n",
    "categories = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "#fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#-----\n",
    "index = range(len(categories))\n",
    "bar_width = 0.35\n",
    "rects = ax1.bar(index, categories, color=['lightcoral','gold', 'lightskyblue', 'yellowgreen'])\n",
    "\n",
    "labels = categories\n",
    "\n",
    "low = min(categories)\n",
    "high = max(categories)\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    ax1.text(x = index[i]-0.1, y = categories[i]+0.025*(high-low), s = labels[i], size = 9)\n",
    "\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_xticklabels(('[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['))\n",
    "ax1.set_ylim([0, high+0.1*(high-low)])\n",
    "\n",
    "#-------\n",
    "\n",
    "sizes = [len(cat1), len(cat2), len(cat3), len(cat4)]\n",
    "labels = '[1, 1]', ']1, 5]', ']5, 10]', '[10, inf['\n",
    "colors = ['lightcoral','gold', 'lightskyblue', 'yellowgreen']\n",
    "explode = (0.1, 0, 0, 0)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "\n",
    "#-------\n",
    "\n",
    "fig.set_size_inches(18, 5)\n",
    "#ax.set_xticklabels(('A', 'B', 'C', 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) How many intermediate societies are there between people and their offshore company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an intermediary ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO, edges with same_name_as, same_company_as, same_id_as,same_as... Merge nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_intermediary_start = (edges_completed.start_type == 'intermediary')\n",
    "mask_not_intermediary_start = (edges_completed.start_type != 'intermediary')\n",
    "mask_intermediary_end = (edges_completed.end_type == 'intermediary')\n",
    "\n",
    "mask_edge_intemerdiary_of = (edges_completed.TYPE == 'intermediary_of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_completed.TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_completed[mask_edge_intemerdiary_of].start_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed[mask_edge_intemerdiary_of].end_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed[mask_intermediary_end].TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed[mask_intermediary_start].TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Is there a correlation between the location of the people and the location of their offshore society?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_diff_start_end_address = edges_completed[mask_country_notna & mask_diff_start_end_address]\\\n",
    ".end_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_q5 = edges_completed[mask_officer_to_entity & mask_start_loc_notna & mask_end_loc_notna][['start_country', 'end_jurisdiction']]\n",
    "edges_q5['sum'] = 1\n",
    "edges_q5.start_country = edges_q5.start_country.str.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_country = []\n",
    "\n",
    "for index, row in edges_q5[(edges_q5.start_country.map(len) > 1)].iterrows():\n",
    "    for country in row['start_country']:\n",
    "        modified_row = row.copy()\n",
    "        modified_row['start_country'] = country + '_GRP'\n",
    "        flattened_country.append(modified_row)\n",
    "\n",
    "edges_q5_flattened = edges_q5[edges_q5['start_country'].map(len) == 1]\n",
    "edges_q5_flattened.start_country = edges_q5_flattened.start_country.apply(lambda arr: arr[0])\n",
    "edges_q5_flattened = edges_q5_flattened.append(flattened_country)\n",
    "      \n",
    "print(len(edges_q5))\n",
    "print(len(edges_q5_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_distrib = edges_q5_flattened.groupby(['start_country', 'end_jurisdiction']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Pie printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pourcentage = 0.05\n",
    "\n",
    "def my_autopct(pct):\n",
    "    \"\"\"\n",
    "    Only print pct if it is more than threshold\n",
    "    \"\"\"\n",
    "    return ('%1.1f%%' % pct) if pct > 100*threshold_pourcentage else ''\n",
    "\n",
    "def my_labels(serie):\n",
    "    \"\"\"\n",
    "    Label is the jurisdiction name except if it's lower than threshold then it's ''\n",
    "    \"\"\"\n",
    "    total = serie.sum().values[0]\n",
    "    return [jurisdictions[row[0]] if row[1].values/total > 0.05 else '' for row in serie.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pie = q5_distrib.xs('GBR', level='start_country').sort_values('sum', ascending = False)\n",
    "labels = my_labels(one_pie)\n",
    "\n",
    "plt.pie(one_pie, radius = 2, labels=labels, autopct=my_autopct, startangle=90, shadow = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_distrib_country = q5_distrib.groupby('start_country').sum()\n",
    "q5_distrib_country = q5_distrib.join(q5_distrib_country, how = 'left', on = 'start_country', rsuffix = '_country')\n",
    "q5_distrib_country['pourcentage'] = q5_distrib_country['sum']/q5_distrib_country['sum_country']\n",
    "q5_distrib_country.groupby('start_country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_scatter_pourc = 0.1\n",
    "threshold_scatter_countries = 10000\n",
    "\n",
    "threshold_mask = (q5_distrib_country['pourcentage'] > threshold_scatter_pourc) \\\n",
    "& (q5_distrib_country['sum_country'] > threshold_scatter_countries)\n",
    "\n",
    "q5_thresholded = q5_distrib_country[threshold_mask]\n",
    "\n",
    "countries_x_scatter =  [val[0] for val in q5_thresholded.index.values]\n",
    "jurisdictions_y_scatter =  [jurisdictions[val[1]] for val in q5_thresholded.index.values]\n",
    "scalars_scatter = q5_thresholded['pourcentage'].values * 100\n",
    "\n",
    "plt.scatter(countries_x_scatter, jurisdictions_y_scatter, scalars_scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see Malta and the British Virgin Islands as constant prefered choice whatever the country of origin of the officer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UP cell : now plot distribution for each country and see if there is always a leader, what is the variance, %of total for top1, BUILD MAP Where you can click on a country and see the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_completed[(edges_completed['start_jurisdiction'] != edges_completed['end_jurisdiction']) \\\n",
    "               & (edges_completed['start_jurisdiction'].notna()) & ((edges_completed['end_jurisdiction'].notna()))]\\\n",
    ".TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe of connected components ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the type of START nodes and of END nodes to see if there is a logic (entity always START node etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we consider the ones not leading to entity as outliers and withdraw them ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : column link is more precise (shareholder of) just not there in bahamas\n",
    "## Should be added with NaN if not there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of start/end nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_start_node = nodes.index.get_level_values(0).isin(start_nodes)\n",
    "mask_end_node = nodes.index.get_level_values(0).isin(end_nodes)\n",
    "\n",
    "start_nodes_type = nodes[mask_start_node].index.get_level_values(\"type\").tolist()\n",
    "end_nodes_type = nodes[mask_end_node].index.get_level_values(\"type\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(start_nodes_type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Address is (almost) never an end node (makes sense condsidering the name of edge is \"address of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(end_nodes_type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So intermediary is (almost) never an end node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity is the only node type to have jurisdiction, we need to link every entity with the other nodes that do contain a country_code. Let's see first if connected components always have the one country_code, and if not quantify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_officer_to_entity[edges_officer_to_entity['TYPE'] == 'officer_of']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use slicers to access a set of values on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot map for 10 biggest countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center(code, coord):\n",
    "    if code == 'USA':\n",
    "        return 38.36320700, -101.989278\n",
    "    if code == 'CAN':\n",
    "        return 56.09433049, -101.989278\n",
    "    lat = 0\n",
    "    lon = 0\n",
    "    count = 0\n",
    "    for elem in coord:\n",
    "        lat += elem[0]\n",
    "        lon += elem[1]\n",
    "        count += 1\n",
    "    return [lon/count, lat/count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data = json.load(open('world.json')) \n",
    "code_to_coord = {}\n",
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    coord = world_data['features'][i]['geometry']['coordinates'][0][0]\n",
    "    code_to_coord[code] = compute_center(code, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries = edges_q5_flattened[edges_q5_flattened['start_country'].apply(lambda x: 'GRP' not in x)].groupby('start_country').size().sort_values(ascending = False)\n",
    "top10 = top_countries[0:11]\n",
    "top10 = top10.drop('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(df):\n",
    "    m = df['sum'].sum()\n",
    "    df['sum'] = df['sum'].apply(lambda x: x*100 / m)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylefunction(feature,df):\n",
    "    return {\n",
    "        'fillOpacity': 1 if feature['id'] in df else 0,\n",
    "        'weight': 0,\n",
    "        'fillColor': 'blue'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "world = folium.Map([2,30], tiles='cartodbpositron', zoom_start = 2.3)\n",
    "world_data = json.load(open('world.json'))  \n",
    "list_countries = top10.index\n",
    "test = []\n",
    "for code in code_to_coord:\n",
    "    if code in list_countries and code in countries:\n",
    "        coord = code_to_coord[code]\n",
    "        df = percentage(q5_distrib.xs(code, level='start_country'))\n",
    "        arrows = {}\n",
    "        weights = {}\n",
    "        for c in df.index:\n",
    "            if c not in test:\n",
    "                test.append(c)\n",
    "            if c in code_to_coord:\n",
    "                arrows[c] = code_to_coord[c]\n",
    "                weights[c] = min(10,df.xs(c)['sum']/2)\n",
    "                \n",
    "        #df = df.rename(index={'VG': 'BVI', 'CY':'CYP', 'IM':'IOM', 'KY':'CYM', 'MT':'MLT', 'NZ':'NZL',\n",
    "        #                      'PA':'PMA', 'SC':'SEY', 'SG':'SGP', 'US':'USA', 'KNA':'KN', 'COOK':'CK',\n",
    "        #                      'CAYMN':'CYM', 'MARSH':'MH', 'MAURI':'MU', 'DUBAI':'AE', 'BERMU':'BM', 'LIBER':'LR',\n",
    "        #                      'CHINA':'CN', 'NETH':'NL', 'BRB':'BB', 'BS':'BAH', 'NEV':'USA', 'WYO':'USA',\n",
    "        #                     'USDE':'USA'})\n",
    "        ##REPLACE VG WITH BVI, CY WITH CYP, IM WITH IOM, KY WITH CYM, MT WITH MLT, NZ WITH NZL, PA WITH PMA\n",
    "        ##SC WITH SEY, SG WITH SGP, US WITH USA, KNA WITH KN, COOK WITH CK, CAYMN WITH CYM, MARSH WITH MH,\n",
    "        ##MAURI WITH MU, DUBAI WITH AE, BERMU WITH BM, LIBER WITH LR, CHINA WITH CN, NETH WITH NL, BRB WITH BB,\n",
    "        ## BS WITH BAH\n",
    "        \n",
    "        marker = folium.Marker(location=coord, icon = folium.Icon(color='red'), popup=countries[code])\n",
    "        layer = folium.GeoJson(world_data,\n",
    "                   name=countries[code],\n",
    "                   style_function = lambda x : stylefunction(x,df.to_dict()['sum'])\n",
    "                      ).add_child(marker)\n",
    "        \n",
    "        for c in arrows:\n",
    "            if c in countries:\n",
    "                line = folium.PolyLine([coord,arrows[c]],weight=max(0.5,weights[c]))\n",
    "                marker_end = folium.Marker(location=arrows[c], popup=countries[c])\n",
    "                layer.add_child(marker_end)\n",
    "            \n",
    "                layer.add_child(line)\n",
    "        \n",
    "        layer.add_to(world)\n",
    "\n",
    "    \n",
    "\n",
    "folium.LayerControl().add_to(world)\n",
    "\n",
    "world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot map with markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pie_chart(code):\n",
    "    one_pie = q5_distrib.xs(code, level='start_country').sort_values('sum', ascending = False)\n",
    "    labels = my_labels(one_pie)\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    colors = ['lightcoral','gold', 'lightskyblue', 'yellowgreen', 'violet', 'lightgray', 'palegreen', 'pink']\n",
    "    plt.pie(one_pie, labels=labels, colors=colors, autopct=my_autopct, startangle=90, shadow = True)\n",
    "    plt.title(countries[code], fontsize=12, fontweight=\"bold\", pad=5)\n",
    "    png = 'pie_{}.png'.format(code)\n",
    "    plt.savefig('figures/'+png, dpi=75)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = top_countries[0:101]\n",
    "top100 = top100.drop('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "world_data = json.load(open('world.json'))\n",
    "\n",
    "world = folium.Map([2,30], tiles='cartodbpositron', zoom_start = 2.3)\n",
    "\n",
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    coord = world_data['features'][i]['geometry']['coordinates'][0][0]\n",
    "    if code in top100:\n",
    "        compute_pie_chart(code)\n",
    "        \n",
    "        png = 'figures/pie_{}.png'.format(code)\n",
    "        encoded = base64.b64encode(open(png, 'rb').read()).decode()\n",
    "    \n",
    "        html = '<img src=\"data:image/png;base64,{}\">'.format\n",
    "        iframe = IFrame(html(encoded), width=400, height=200)\n",
    "        popup = folium.Popup(iframe, max_width=2650)\n",
    "    \n",
    "        folium.Marker(location=compute_center(code,coord), popup=popup).add_to(world)\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data_id = []\n",
    "for i in range(len(world_data['features'])):\n",
    "    code = world_data['features'][i]['id']\n",
    "    world_data_id.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
