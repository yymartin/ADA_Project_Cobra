{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://www.occrp.org/en/panamapapers/database\n",
    "# TRUMP OFFSHORE INC. is good example to see all entities interacting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames / paths\n",
    "\n",
    "The data is separated for every leak source. For each leak source there is a folder containing the nodes of the graph, that can be of different types : <i>intermediary, officer, entity, address</i> (and <i>other</i> for paradise papers only). The folder also contains the edges of this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahamas_folder = \"bahamas/\"\n",
    "panama_folder = \"panama/\"\n",
    "paradise_folder = \"paradise/\"\n",
    "offshore_folder = \"offshore/\"\n",
    "\n",
    "sources_names = [bahamas_folder[:-1], panama_folder[:-1], paradise_folder[:-1], offshore_folder[:-1]]\n",
    "\n",
    "panama_name = panama_folder + \"panama_papers\"\n",
    "paradise_name = paradise_folder + \"paradise_papers\"\n",
    "offshore_name = offshore_folder + \"offshore_leaks\"\n",
    "bahamas_name = bahamas_folder + \"bahamas_leaks\"\n",
    "\n",
    "edges_name = \".edges\"\n",
    "nodes_name = \".nodes.\"\n",
    "\n",
    "address_name = \"address\"\n",
    "intermediary_name = \"intermediary\"\n",
    "officer_name = \"officer\"\n",
    "entity_name = \"entity\"\n",
    "others_name = \"other\" # Only for paradise paper there is this extra entity\n",
    "\n",
    "usual_entity_names = [address_name, intermediary_name, officer_name, entity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build local storage\n",
    "\n",
    "We store data in dictionnaries that map each leak source to its content, which is a dictionnary that maps each type of entity to the Dataframe containing its values. For example <b>d_sources[\"bahamas\"][\"officer\"]</b> is the Dataframe of officers coming from the bahamas leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(filename) :\n",
    "    \"\"\" To have same rules when reading data from csv \"\"\"\n",
    "    return pd.read_csv(filename, dtype = str)\n",
    "\n",
    "def build_dict(source_name):\n",
    "    \"\"\"\n",
    "    Create a dictionnary for a certain source_name (among : Panama papers, Paradise papers...)\n",
    "    that maps to each entity name (among : Officer, Intermediary, Address...)\n",
    "    the content of the csv from source_name for this entity\n",
    "    \"\"\"\n",
    "    d = {en : my_read_csv(source_name + nodes_name + en + \".csv\") for en in usual_entity_names}\n",
    "    \n",
    "    if source_name == paradise_name: # Extra \"other\" entity in paradise papers\n",
    "        d[others_name] = my_read_csv(source_name + nodes_name + others_name + \".csv\")\n",
    "    \n",
    "    #Add edges\n",
    "    d[\"edges\"] = my_read_csv(source_name + edges_name + \".csv\")\n",
    "              \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionnary, that maps each source its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources = dict()\n",
    "d_sources[\"bahamas\"] = build_dict(bahamas_name)\n",
    "d_sources[\"panama\"] = build_dict(panama_name)\n",
    "d_sources[\"paradise\"] = build_dict(paradise_name)\n",
    "d_sources[\"offshore\"] = build_dict(offshore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources['panama']['entity'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some coloring for printing\n",
    "\n",
    "Keep the same coloring during the project, it makes data very easily readable once you get familiar with the coloring !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = '\\033[1m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RED = '\\033[91m'\n",
    "END = '\\033[0m'\n",
    "\n",
    "color_dict = dict()\n",
    "color_dict[\"bahamas\"] = YELLOW\n",
    "color_dict[\"paradise\"] = GREEN\n",
    "color_dict[\"panama\"] = RED\n",
    "color_dict[\"offshore\"] = BLUE\n",
    "\n",
    "def color(str):\n",
    "    \"\"\"\n",
    "    Returns the str given in the color of the source it is from \n",
    "    (the str must contain source name)\n",
    "    \"\"\"\n",
    "    for source in color_dict.keys():\n",
    "        if source in str:\n",
    "            return color_dict[source] + str + END \n",
    "        \n",
    "    return BOLD + str + END #Default color is BOLD\n",
    "\n",
    "for name, _ in color_dict.items():\n",
    "    print(color(name))\n",
    "print(color(\"Unknown source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what data source misses which column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    for source_compare, dict_data_compare in d_sources.items():\n",
    "        print(\"\\n\", color(source_compare), \"missing columns from source :\", color(source))\n",
    "        for entity in usual_entity_names:\n",
    "            missing_columns = []\n",
    "            for col in dict_data[entity].columns:\n",
    "                if not col in dict_data_compare[entity].columns:\n",
    "                    missing_columns.append(col)\n",
    "            if(len(missing_columns) > 0):\n",
    "                print(\"Node type\", entity, \"misses\", len(missing_columns), \"columns, namely : \", missing_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that <span style=\"color:orange\">bahamas</span> is the most \"complete\" source, in the sense it is the one that has the biggest number of columns missing in the others. We will therefore use it to explore the content of columns. *'inactivation_date'* and  *'struck_off_date'* columns from entity will then be explored in <span style=\"color:red\">panama</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special case : Paradise paper, <i>other</i> node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sources[\"paradise\"][\"other\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SourceID in different sources\n",
    "\n",
    "We see paradise papers is the only source that has different sourceID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dict_data in d_sources.items():\n",
    "    print(\"\\nSource :\", color(source))\n",
    "    for entity in usual_entity_names:\n",
    "        value_count =  dict_data[entity][\"sourceID\"].value_counts()\n",
    "        print(\"Node :\", entity, len(value_count), \"different sourceID :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if node_id is a good index for Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_node_id = pd.Series()\n",
    "\n",
    "for source, dict_data in d_sources.items():\n",
    "    merged_node_id_source = pd.Series()\n",
    "    for entity in usual_entity_names:\n",
    "        \n",
    "        merged_node_id_source = merged_node_id_source.append(dict_data[entity][\"node_id\"], ignore_index = True)\n",
    "        \n",
    "        if not dict_data[entity][\"node_id\"].is_unique:\n",
    "            print(\"node_id isn't unique for source\", color(source, \"node\", entity))\n",
    "                  \n",
    "    if not merged_node_id_source.is_unique:\n",
    "        print(\"node_id isn't unique between nodes from source\", color(source))\n",
    "    \n",
    "    merged_node_id = merged_node_id.append(merged_node_id_source.drop_duplicates())\n",
    "\n",
    "if merged_node_id.is_unique:\n",
    "    print(\"node_id is unique between unique nodes from all sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each node type indepently node_id is a good index. Therefore (node_id, node_type) could be a good index (node_type being amond officer, intermediary...)\n",
    "\n",
    "Now explore nodes with same node_id in offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(usual_entity_names)):\n",
    "    for j in range(i+1, len(usual_entity_names)):\n",
    "\n",
    "        left_node = usual_entity_names[i]\n",
    "        node = usual_entity_names[j]\n",
    "        print(color(left_node), color(node))\n",
    "        \n",
    "        if left_node != node:\n",
    "\n",
    "            left = d_sources[\"offshore\"][left_node].set_index(\"node_id\")\n",
    "            right = d_sources[\"offshore\"][node].set_index(\"node_id\")\n",
    "\n",
    "            intersection = left.join(right, on = \"node_id\", how = 'inner', \\\n",
    "                                     lsuffix = \"_\" + left_node,rsuffix = \"_\" + node)\n",
    "\n",
    "            if not intersection.empty:\n",
    "                print(\"Intersection of\", color(left_node), \"and\", color(node), \"count is :\")\n",
    "                print(intersection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the intersection on offshore is between officer and intermediary nodes. Let's see if they are the same values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = d_sources[\"offshore\"][\"officer\"].set_index(\"node_id\")\n",
    "right = d_sources[\"offshore\"][\"intermediary\"].set_index(\"node_id\")\n",
    "\n",
    "intersection = left.join(right, on = \"node_id\", how = 'inner', lsuffix = \"_officer\",rsuffix = \"_interm\")\n",
    "\n",
    "intersection.loc[intersection[\"name_officer\"]!= intersection[\"name_interm\"]].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we understand that if someone appears in two different node types, it means he has two roles. This is why in further analysis we will store the pair (node_id, role) as index, because it is unique. We have to add a column to nodes, containing the node type, let's call it label. We saw in the column exploration that bahamas has an equivalent column *labels(n)*, that the other's don't, we'll rename it to *label*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in [\"paradise\", \"offshore\", \"panama\"]:\n",
    "    for role in usual_entity_names:\n",
    "        d_sources[source][role][\"label\"] = role\n",
    "\n",
    "for role in usual_entity_names:\n",
    "        d_sources[\"bahamas\"][role].rename(columns={\"labels(n)\": \"label\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check bahamas label is consistent (only one value for each type of node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for role in usual_entity_names:\n",
    "    print(color(\"bahamas\"), role, \"number of different values :\", d_sources['bahamas'][role][\"label\"].value_counts().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_clean = copy.deepcopy(d_sources)\n",
    "d_clean[\"bahamas\"]['address'] = d_sources[\"bahamas\"]['address'][['country_codes', 'node_id']]\n",
    "d_clean[\"bahamas\"]['address']['file'] = 'bahamas'\n",
    "d_clean[\"bahamas\"]['address']['type'] = 'address'\n",
    "\n",
    "d_clean[\"panama\"]['address'] = d_sources[\"panama\"]['address'][['country_codes', 'node_id']]\n",
    "d_clean[\"panama\"]['address']['file'] = 'panama'\n",
    "d_clean[\"panama\"]['address']['type'] = 'address'\n",
    "\n",
    "d_clean[\"paradise\"]['address'] = d_sources[\"paradise\"]['address'][['country_codes', 'node_id']]\n",
    "d_clean[\"paradise\"]['address']['file'] = 'paradise'\n",
    "d_clean[\"paradise\"]['address']['type'] = 'address'\n",
    "\n",
    "d_clean[\"offshore\"]['address'] = d_sources[\"offshore\"]['address'][['country_codes', 'node_id']]\n",
    "d_clean[\"offshore\"]['address']['file'] = 'offshore'\n",
    "d_clean[\"offshore\"]['address']['type'] = 'address'\n",
    "\n",
    "\n",
    "d_clean['bahamas']['entity'] = d_sources['bahamas']['entity'][['node_id','name','jurisdiction','incorporation_date']]\n",
    "d_clean[\"bahamas\"]['entity']['file'] = 'bahamas'\n",
    "d_clean[\"bahamas\"]['entity']['type'] = 'entity'\n",
    "\n",
    "d_clean['panama']['entity'] = d_sources['panama']['entity'][['node_id','name','jurisdiction','country_codes','incorporation_date']]\n",
    "d_clean[\"panama\"]['entity']['file'] = 'panama'\n",
    "d_clean[\"panama\"]['entity']['type'] = 'entity'\n",
    "\n",
    "d_clean['paradise']['entity'] = d_sources['paradise']['entity'][['node_id', 'name','jurisdiction','country_codes','incorporation_date']]\n",
    "d_clean[\"paradise\"]['entity']['file'] = 'paradise'\n",
    "d_clean[\"paradise\"]['entity']['type'] = 'entity'\n",
    "\n",
    "d_clean['offshore']['entity'] = d_sources['offshore']['entity'][['node_id', 'name','jurisdiction','country_codes','incorporation_date']]\n",
    "d_clean[\"offshore\"]['entity']['file'] = 'offshore'\n",
    "d_clean[\"offshore\"]['entity']['type'] = 'entity'\n",
    "\n",
    "d_clean['bahamas']['intermediary'] = d_sources['bahamas']['intermediary'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"bahamas\"]['intermediary']['file'] = 'bahamas'\n",
    "d_clean[\"bahamas\"]['intermediary']['type'] = 'intermediary'\n",
    "\n",
    "d_clean['panama']['intermediary'] = d_sources['panama']['intermediary'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"panama\"]['intermediary']['file'] = 'panama'\n",
    "d_clean[\"panama\"]['intermediary']['type'] = 'intermediary'\n",
    "\n",
    "d_clean['paradise']['intermediary'] = d_sources['paradise']['intermediary'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"paradise\"]['intermediary']['file'] = 'paradise'\n",
    "d_clean[\"paradise\"]['intermediary']['type'] = 'intermediary'\n",
    "\n",
    "d_clean['offshore']['intermediary'] = d_sources['offshore']['intermediary'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"offshore\"]['intermediary']['file'] = 'offshore'\n",
    "d_clean[\"offshore\"]['intermediary']['type'] = 'intermediary'\n",
    "\n",
    "d_clean['bahamas']['officer'] = d_sources['bahamas']['officer'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"bahamas\"]['officer']['file'] = 'bahamas'\n",
    "d_clean[\"bahamas\"]['officer']['type'] = 'officer'\n",
    "\n",
    "d_clean['panama']['officer'] = d_sources['panama']['officer'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"panama\"]['officer']['file'] = 'panama'\n",
    "d_clean[\"panama\"]['officer']['type'] = 'officer'\n",
    "\n",
    "d_clean['paradise']['officer'] = d_sources['paradise']['officer'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"paradise\"]['officer']['file'] = 'paradise'\n",
    "d_clean[\"paradise\"]['officer']['type'] = 'officer'\n",
    "\n",
    "d_clean['offshore']['officer'] = d_sources['offshore']['officer'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"offshore\"]['officer']['file'] = 'offshore'\n",
    "d_clean[\"offshore\"]['officer']['type'] = 'officer'\n",
    "\n",
    "d_clean['paradise']['other'] = d_sources['paradise']['other'][['node_id', 'country_codes','name']]\n",
    "d_clean[\"paradise\"]['other']['file'] = 'paradise'\n",
    "d_clean[\"paradise\"]['other']['type'] = 'other'\n",
    "\n",
    "d_clean['bahamas']['edges'] = d_sources['bahamas']['edges'][['node_1','node_2', 'rel_type', 'start_date', 'end_date']]\n",
    "d_clean['bahamas']['edges']['file'] = 'bahamas'\n",
    "d_clean['panama']['edges'] = d_sources['panama']['edges'][['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date']]\n",
    "d_clean['panama']['edges']['file'] = 'panama'\n",
    "d_clean['paradise']['edges'] = d_sources['paradise']['edges'][['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date']]\n",
    "d_clean['paradise']['edges']['file'] = 'paradise'\n",
    "d_clean['offshore']['edges'] = d_sources['offshore']['edges'][['START_ID', 'END_ID', 'TYPE', 'start_date', 'end_date']]\n",
    "d_clean['offshore']['edges']['file'] = 'offshore'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create node dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(columns=['node_id','file','type','name','country_codes', 'jurisdiction', 'incorporation_date'])\n",
    "nodes = nodes.append([d_clean[\"bahamas\"]['address'],d_clean[\"panama\"]['address'],d_clean[\"paradise\"]['address'],d_clean[\"offshore\"]['address']], sort=False)\n",
    "nodes = nodes.append([d_clean[\"bahamas\"]['entity'],d_clean[\"panama\"]['entity'],d_clean[\"paradise\"]['entity'],d_clean[\"offshore\"]['entity']], sort=False)\n",
    "nodes = nodes.append([d_clean[\"bahamas\"]['intermediary'],d_clean[\"panama\"]['intermediary'],d_clean[\"paradise\"]['intermediary'],d_clean[\"offshore\"]['intermediary']], sort=False)\n",
    "nodes = nodes.append([d_clean[\"bahamas\"]['officer'],d_clean[\"panama\"]['officer'],d_clean[\"paradise\"]['officer'],d_clean[\"offshore\"]['officer']], sort=False)\n",
    "nodes = nodes.append(d_clean['paradise']['other'], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create edges dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_clean['bahamas']['edges'].columns=['START_ID', 'END_ID', 'rel_type', 'start_date', 'end_date','file']\n",
    "\n",
    "edges = pd.DataFrame(columns=['START_ID', 'END_ID', 'rel_type', 'start_date', 'end_date','file'])\n",
    "edges = edges.append([d_clean['bahamas']['edges'], d_clean['panama']['edges'], d_clean['paradise']['edges'], d_clean['offshore']['edges']], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for countries and jurisdictions\n",
    "\n",
    "These dictionaries map the abrevation of countries to their full name, this way we can drop the longer column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = dict(zip(d_sources['bahamas']['address']['country_codes'], d_sources['bahamas']['address']['countries']))\n",
    "countries.update(dict(zip(d_sources['panama']['address']['country_codes'], d_sources['panama']['address']['countries'])))\n",
    "countries.update(dict(zip(d_sources['paradise']['address']['country_codes'], d_sources['paradise']['address']['countries'])))\n",
    "countries.update(dict(zip(d_sources['offshore']['address']['country_codes'], d_sources['offshore']['address']['countries'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisdictions = dict(zip(d_sources['bahamas']['entity']['jurisdiction'], d_sources['bahamas']['entity']['jurisdiction_description']))\n",
    "jurisdictions.update(dict(zip(d_sources['panama']['entity']['jurisdiction'], d_sources['panama']['entity']['jurisdiction_description'])))\n",
    "jurisdictions.update(dict(zip(d_sources['paradise']['entity']['jurisdiction'], d_sources['paradise']['entity']['jurisdiction_description'])))\n",
    "jurisdictions.update(dict(zip(d_sources['offshore']['entity']['jurisdiction'], d_sources['offshore']['entity']['jurisdiction_description'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : \n",
    "- some have file with NaN ???\n",
    "- What to do with those with no country_code ?\n",
    "- What to do with those with no incorporation date ? \n",
    "- Define difference between jurisdiction and country_code\n",
    "- definde node_id/type as index\n",
    "- keep validity date ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges with no end_date are still true until \"date of validity\"\n",
    "- Turn start/end date to DATE format, check outliers/typos\n",
    "- Study duff between rel_type and TYPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
